{"meta":{"title":"Zhang's Blog","subtitle":"","description":"","author":"Junru Zhang","url":"http://zhangdeweb.site","root":"/"},"pages":[{"title":"categories","date":"2025-01-01T16:12:13.100Z","updated":"2025-01-01T16:12:13.100Z","comments":false,"path":"categories/index.html","permalink":"http://zhangdeweb.site/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2025-01-01T16:12:13.100Z","updated":"2025-01-01T16:12:13.100Z","comments":false,"path":"tags/index.html","permalink":"http://zhangdeweb.site/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Tasting Notes","slug":"coffee","date":"2025-01-01T16:12:13.100Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2025/01/02/coffee/","link":"","permalink":"http://zhangdeweb.site/2025/01/02/coffee/","excerpt":"","text":"个人评价标准： 易于获得的好风味，才是真的好风味。对我而言，咖啡更多作为功能性饮料，经过实际对比和综合考量，便携意式咖啡机是我的主要器材，以便我在多种场合高效制取咖啡。 出于同样的原因，我希望我的咖啡豆不仅在良好工况下可以制取出高品质的咖啡，在由于各种原因导致的不良工况下，如水温不足或过高、研磨过细或过粗、不筛细粉、暇疵豆等，仍能保持一定的质量或基本特征。纵然，在闲暇时通过精心的准备可以获得极致的风味也是一件美事，但是在我不精心控制它的时候，它的表现更为重要。 对于品质的基本要求，我将所有参数分为两方面，入口前的感受和入口后的感受。入口前的考察包括从见到豆子开始到咖啡入口之前到所有感受，以是否刺激食欲为标准，主要考察香气，观感次之，手感再次；入口后的感受考察咖啡入口后到代谢完毕的所有感受，以饮用咖啡是否带来愉悦情绪为标准，主要考察咖啡入口时的味觉、嗅觉刺激，咖啡因的精神刺激次之，对肠道的刺激再次。为便于表述，使用两个考察阶段最重要的感受指代两个考察阶段，即入口前考察统称为香气考察，入口后考察统称为口感考察。 趣味性。稳定的品质是我对咖啡豆的第一要求，而丰富的变化也是咖啡豆有趣的地方，在不同工况下可以得到风味变化的高品质咖啡将会给咖啡豆带来更多可玩性。此处主要指主动调整的工况，例如研磨度调整，冰咖啡，冷萃，浸泡，手冲，勾兑其他原料等。 交叉验证。个人认为前两条要求已经渐增地给出了足够苛刻的要求，但在如此丰富的咖啡豆产品中，或许仍然会有一定数量的咖啡豆可以比较充分的满足我的要求，而对其进一步的挑剔或许并不能体现出他们的差异，因而在此我再加入一个难以满足且确有需求的要求，即咖啡对我而言除了功能性和趣味性之外，也具有一点社交属性。我也很喜欢和他人分享我的咖啡，而不同人的口味不同，能让对咖啡有不同程度了解的人都有比较正面的评价，才是好的咖啡豆。 以上三个方面，即功能性（稳定性）、趣味性、社交性，我的评价要求是依次提高的，亦即对同一款豆子，靠后的需求应该在满足靠前的需求的基础上得到满足。3.5为基准评分，即“感受一般”，4分以“没有不喜欢的点”为标准，依据特点数量和喜好程度在此基础上加分。稳定性一般不做变化，故小于4.5分时酌情扣分；趣味性和社交性不参与基础综合打分。 在以下评价中，如未提及，所有咖啡基于WACACO Exagrind磨豆机14刻度，在WACACO Picopresso便携意式咖啡机中，使用12g粉碗，全段液体制备浓缩，后加入适量水稀释而成。 2024 [!coffee]- ## 爱窝香啡 酒香哥伦比亚 天堂92庄园 酵母酒桶发酵 #medium_roast 香气中甜和醇是主导元素，伴有一定酵酸，因而表现为奶糖、木本植物、酒香和成熟水果香气。 基本信息 4.1 （zzc：4.2） 产地：哥伦比亚, 考卡Cauca省, 天堂92庄园, 海拔1950m 豆种：COLOMBIA 哥伦比亚, 玛卡瑞纳地块 微批次, 全红果采收 处理：中度烘焙，特殊处理（啤酒酵母+朗姆酒桶发酵） 参考风味：酒香，红樱桃，朗姆酒，橡木，甘草，红色花香，青提，甜李，可可，啤酒花 个人感受 香气：4.4 总体而言香气令人愉悦，隔着包装可以闻到焦脆的成熟香气和一些成熟的水果香气，香气余韵醇，没有焦苦感或尖酸的醋味，开袋后有些许发酵味，但空气稀释后发酵味表现为成熟香气。色泽正常咖啡颜色，豆大。研磨时香气香甜，无酸味，冲泡时排气程度较轻，咖啡液色泽深棕，前段味道高温时略酸，轻微刺激感，后段油脂味道中等较低，混合后酸下降，静置有酒香，果香，木桶味道，轻度而不令人厌恶的发酵酸味。 口感：3.9 浓缩口感入口甜，但紧接着是酸，苦，有湿木头般的中药感受，后接木桶果酒等其他香气，余韵长且令人愉悦，稀释后甜感提升，酸感下降，味道更有层次，煮软烂木头的味道转变为浸泡紧实木头的味道，果、酒香气突出，醇感和甜感明显，因而有奶糖风味，余韵缩短。 在酸感之后，酒香之前，有一定的中药感。 稳定性：4.5 7月20日装袋，8月8日开封。冲泡品质较稳定。8.14，香气尚好，但冲泡时后段萃取速度慢，导致萃取时间长，则口感苦涩，无香气，稀释后仍然如此，鉴定为过萃。 尝试用18g粉碗做双份浓缩，量大易于控制，发现即便萃取最后油脂味道依然较轻。 变化：3.5 出于部分时刻偏酸的口感，尝试研磨加细到13，研磨后香气略苦涩，冲泡后段咖啡液油脂味道中低，甚至仍带轻微酸感，给人一种干燥的草本植物的感受，香气酸感低，醇感加，有花香。浓缩香气酸感减轻，苦涩感略增加，酸气的位置从第一感受后移至中后部。口感甜、酸、苦均衡，苦味路多，气息无酸，醇感为主。稀释后，酸近无，苦涩重，有些许杏仁感，有黑可可，皮革等气息，鉴定为过萃。 有一个有点搞笑的地方，是它冲过咖啡之后的那个粉饼很香，不知道是水温不足导致这些风味物质没有被萃取出来，加上萃取时间过长导致苦涩感增重，从而喝的苦而闻着咖啡渣很香。 [!coffee]- ## 爱窝香啡 蓝山一号风味拼配 试用小样 #medium_roast醇厚温和的米浆，很难有酸感，这可以是优点，但是也因此缺少了最丰富的一部分变化。值得注意的一点是，好像劲挺大的，喝下去立刻就有上头的感觉，不像其他的还需要等一会，这里面是不是拼了罗豆，如果是这样的话，那真是速溶风味了。 基本信息 3.4 产地：中南美高海拔产区 豆种：阿拉比卡 处理：中度烘焙，水洗处理 参考风味：无 个人感受 香气：3.9 很难讲能闻到什么酸味，醇香而不苦涩，甚至在醇感之前还有一些花香，收尾有一点烟熏感，综合起来，嗯，有一点点像放了一些香料后的炸鸡，哈哈。冲泡时候前段香感大体同上，后段油脂味道略重，但注意到前段出水量似乎比后段大不少，所以混合后仅有少量短暂的油脂味。 口感：3.2 浓缩入口略酸，但很快消失，成为醇感，苦涩少，香气少。稀释后口感顺滑，第一感受就是这个，顺滑，之后略有酸，香气以醇为主，有些许坚果、可可味道，综合起来有一种米浆的感受。无论是浓缩还是稀释后，无论是香气还是口感，没有任何刺激性的感受，都是温和的。可以尝到一些“雀巢”感，想到速溶咖啡大多以蓝山风味作为优化方向，这种温和感确实可能更容易被更多人接受。 稳定性：未测评 仅15g密封包装小样。 变化：未测评 [!coffee]- ## 爱窝香啡 巴西圣保罗皇后庄园黄波旁甜蜜日晒 小样 #medium_roast葡萄干和炒瓜子的香气倒是挺特别的，但是这东西不能入口，或者说容易较过萃。 基本信息 3.2 （zzc：3.7） 产地：圣保罗Sao Paulo莫吉安娜 Mogiana皇后庄园Fazenda Rainha 海拔1150-1350m 豆种：黄波旁 FC SS NY.2 处理：中度烘焙 参考风味：无 个人感受 香气：3.6 （zzc：3.7） 醇感和焦香丰溢，酸感轻，有坚果、巧克力的香气，苦感有但轻，后段有油脂感，稀释后有茶感的香和苦涩。研磨时醇、甜、酸适中，有焦香。果香有，但轻、熟，且靠后。综合感受有奶油、炒焦糖瓜子、水果凤梨的感受。冲泡时候，前段有焦糖、坚果、葡萄干香气，后段有部分烟熏和焦糊感，故最后段弃之不用。 口感：3.0 （zzc：3.5） 浓缩口感只有从头到尾的强烈苦。稀释四倍后仍没有什么清香，苦感仍在。或许过萃了吧，但已经弃了后段了，所以没什么更多能解释的了。 稳定性：未测评 变化：未测评 [!coffee]- ## 爱窝香啡 威士忌香草精品意式拼配豆 #deep_roast香气浓郁香甜，太妃糖感受令人愉悦，酒香尾韵绵长；口感顺滑，但酸感少，苦感无法被压制。 基本信息 3.9 产地：洪都拉斯Honduras伦比拉Lempira茉罗德庄园 MOROD MANOR； 豆种：Caturra 等级SHG 处理：水洗处理，威士忌酒桶发酵处理，中深度烘焙 产地：乌干达 布吉苏Bugisu 圣保罗皇后庄园Fazenda Rainha 豆种：黄波旁 Yellow bourbon 等级AA 处理：甜蜜日晒FC SS NY.2，中深度烘焙 参考风味：入口浓烈的威士忌酒香和明显香草香气，芝士奶油风味，坚果，太妃糖巧克力奶油股的触感，均衡感好，油脂稳定，层次清晰，果敢适中，丰盈饱满，圆润丝滑回甘，余韵持久悠长。冷萃手冲也有不俗表现，推荐用法：奶咖、美式、浓缩、冷萃、手冲高端咖啡厅意式用豆，意式功夫浓缩发烧友，咖啡竞赛推荐用豆。 个人感受 香气：4.6 酒香突出，甜感明显，有香草奶茶、水果感受，醇感靠后，有巧克力感，酸感弱，苦感无。研磨时香气如上，有坚果和少部分酸感。冲泡时浓厚的太妃糖香气，一点点不令人厌烦的烟熏味道增加层次，太妃糖衍生的坚果木质味道、水果、奶油等香气。冲泡前段坚果香气明显，后接奶油、香草等香气，最后有少部分焦糖、烘焙饼干的感受；后段木制香调，有明显烟熏感，能有苦的感受。 口感：3.4 浓缩甜感、酸感、苦适度，香气明显，回香甘甜，余韵长，后段香气尚存，有烟熏焦糊苦涩感受。稀释后苦感适度，甜感强，口感顺滑，醇感浓厚，酸轻，有些蓝山感，但香气不同。酒香在后部，甘甜，绵长。但在酒感之前有一点点烟熏感，结合苦感有一点点不好的感受，但稍纵即逝，不知道能不能用水温或什么平衡一下。 稳定性：4.5 （？） 7.20封装，8.25开封，8.29香气减弱，苦感增加。可能赏味期已过。 变化：3.5 考虑降低水温、仅取头段来避免过萃，但仍然有些许苦感，应该是豆子本身烘焙的问题，不知道是不是店家烘焙时不小心把部分豆子烘过头了。 考虑研磨加粗到15，双份18g粉碗加滤纸萃取初段，香气尚存，难掩苦味，可能已经过了最佳赏味期。 对它的香气不死心，减轻调整为中度烘焙，研磨14，双份粉碗萃取。苦感少了，但是香气也少了，油脂变得格外丰富，从头到尾全是油脂。最后香苦比貌似没什么变化。盖棺定论，偏科生。 [!coffee]- ## 泰摩 哥伦比亚-榛 #medium_roast泰摩作为精品豆里的大品牌，品质非常稳定、均衡，参考风味也非常准确。甘薯香气明显，酸适度，低苦，各项表现均衡，无明显缺点。但味型单一，缺少变化，也无法有更高的期待。简单来说就是标准咖啡+甘薯香气。但这种强大的稳定，单说过萃不苦这一点，就是很难得的，所以可以称之为优秀的咖啡豆。 基本信息 4.1 产地：哥伦比亚 卡乌卡产区 1700-1800 豆种：阿拉比卡 处理：水洗、中度烘焙 参考风味：榛果、杏仁、焦糖、苹果。卡乌卡产区的豆子风味浓郁，甜度、纯净度高，生豆有青草清香，烘焙后有焦糖、甘薯、坚果风味。豆种是哥伦比亚少有的埃塞原生种，和耶加一样有不均匀的颗粒。哥国豆一般品质稳定，口味均衡但缺少特点，本豆打破这样的格局，有适度的果酸，带来南美特色。 个人感受 香气：4.1 开袋气息是浓郁的烤红薯香气，而且是很甜的那种红心、溏心红薯，还有栗子的香气，末尾有烤坚果的香气，稀释后有一定的花果芬芳。冲泡前段有明显酸感，后段有焦糖醇感，混合后酸度适中，甜感中上，无明显的苦感。 口感：4.0 浓缩甜适度、酸略多、苦轻微，即便是末段苦味也可以接受，不过这种感受可能也与最近喝的上一款咖啡太苦有关；甘薯香气明显，伴随坚果香气，回味有焦糖的醇感。稀释后无苦味，甜感适度，酸感中上，略有醇感，舌面略有涩感。回味坚果感受明显，但缺少余韵和变化。 稳定性：4.9 8.28封装，9.2开封，9.11香气轻微减弱但是口感没有大幅变差，苦涩少导致酸的刺激更明显了。 变化：3.5更改研磨，变化不大。 [!coffee]- ## 爱窝香啡 慢速酵母厌氧水洗古齐乌拉加红草莓 #city_roast花果香气浓郁，但轻浮，缺少衍生，加热后的花果香气并不会带来愉悦感受。酸甜平衡良好，苦感低，醇感近无，也映衬了其香气表现。劣质香薰味道令人作呕。 基本信息 3.2 产地：埃塞俄比亚古齐乌拉加Guji Uraga ，TEBE HARRO ZENASH庄园，2220-2280 豆种：JARC74110，74112 人工采收 G1 处理：慢速酵母厌氧发酵水洗处理 参考风味：强烈的花果香，草莓、白桃、甜瓜、葡萄柚、柑橘、红茶、奶油、水果。香气浓郁，口感干净，香醇馥郁的埃塞产区独有的鲜明特征，慢速厌氧发酵带来草莓甜香，爆汁水果调性，浪漫花香，浓郁果香，干香拉满，尾韵悠长，入口难忘。 个人感受 香气：3.0 干香有草莓香气，果香很足，有花香芬芳，浅色瓜果为主，酸感有，甜感有，醇感淡。研磨后有一定的茶感。冲泡全程带有饱满的花果芬芳，酸甜感组成草莓感受。尾段油脂感受近无。 口感：3.3 浓缩舌尖甜感有，酸感重，苦感很低。轻浮的花果香，没什么变化，尾韵伴随一些油腻感，有点像某种廉价的香薰。稀释后酸甜感均衡适度，有酵酸感受，香气简单丰沛。 稳定性： 9.2封袋，9.13开包。劣质香薰味道久久不散，并不好喝。 变化： 冷萃也不好喝，劣质香薰味道贯穿始终。 [!coffee]- ## 治光师 香料地午餐 产地特种版 阿拉比卡意式 拼配 #medium_roast 基本信息 产地：危地马拉、坦桑尼亚、埃塞俄比亚、肯尼亚、巴布亚新几内亚 五产区拼配 豆种：阿拉比卡 处理：日晒，快速、中度烘焙 参考风味：热带气息，丁香、肉桂风味，建议93度热水，9bar，18g，26-28s萃取，32-35g液。最佳风味期限：生产两周后，开袋后45天饮用完成。 个人感受 香气： 醇感重，有香甜气息，浓缩后段风味寡淡。 口感： 浓缩后段苦，寡淡。稀释后酸，有红薯风味？苦低，有甜感。回味有些复杂，暂时不好形容。 稳定性： 9.21封装，10.6开封。 变化： [!coffee]- ## 威士忌 格兰塔雷特 GLENTURRET HIGH LAND SINGLE MALT SCOTCH WHISKY aged 10 years peat smoked 甜感强，香，有烟熏风味。好喝。 [!coffee]- ## 威士忌 格兰杰雪莉桶 GLENMORANGIE HIGHLAND SINGLE MALT SCOTCH WHISKY 12 寡淡，不好喝。 [!coffee]- ## 葡萄酒 CHATEAU GRUAUD LAROSE 2020法国 圣朱利安SAINT-JULIEN 很香，甜感强但不腻，果香丰富，有奶酪香气，层次丰富，回味有浆果、凤梨的味道。很好喝。","categories":[{"name":"diaries","slug":"diaries","permalink":"http://zhangdeweb.site/categories/diaries/"}],"tags":[{"name":"coffee","slug":"coffee","permalink":"http://zhangdeweb.site/tags/coffee/"},{"name":"taste_note","slug":"taste-note","permalink":"http://zhangdeweb.site/tags/taste-note/"}]},{"title":"Daily work","slug":"diary/autodaily","date":"2025-01-01T16:12:13.100Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2025/01/02/diary/autodaily/","link":"","permalink":"http://zhangdeweb.site/2025/01/02/diary/autodaily/","excerpt":"","text":"automatic update via todo-list 2024&#x2F;08&#x2F;10 (10.4h)Doing tasksDone tasks ‼edit my blog and organize files (10.4h) Todo tasks in this week ‼模型描述: build the zero-evolution and add para; vim zhongqi_latex&#x2F;src&#x2F;03_model.tex and 00-daily*.md though learning model adding and config in revbayes make a model list for revbayes P3 输入文件格式，即数据来源 研究一下qmk，把轨迹球移动之后自动切换到第三层这件事给取消掉，再看看有什么可以配置玩的。 align with magus？ ON dating: run muscle in align and test mode via its online document. alignment is not much usefull on such huge dataset, why? or these gaps is normal on such taxa_num datasets? Filt them into small dataset will influent the pps_result? Can I get any acceptable pps_result? current running : mito_dataset. mft-ginsi&#x2F;magus on dating; rb_pps on tower. check the result Memo &amp; Comments #char_type:335069 edit blog in autodaily, add TODOLIST&#x2F;WISHLIST in hexo, edit via obsidian. Write my thesis paper via mdbook; Maybe I should use mdbook instead of beamer for presentation, not hexo for documentation, maybe. #mdbook #hexo #todo #obsidian #blog Using todo list and hexo-blog via obsidian for daily work, using mdbook for presentation. 2024&#x2F;08&#x2F;10 (5.8h)Doing tasksDone tasks align with #magus ？ (0.0h) #alignment current running : mito_dataset. mft-ginsi&#x2F;magus on dating; rb_pps on tower. check the result (2.3h) #alignment my vpn maybe broken, check it. (0.5h) config my workflow consist of todo&#x2F;hexo&#x2F;obsidian&#x2F;mdbook&#x2F;blog-README (3.0h) Todo tasks in this week ‼模型描述: build the zero-evolution and add para; vim zhongqi_latex&#x2F;src&#x2F;03_model.tex and 00-daily*.md though learning model adding and config in revbayes make a model list for revbayes P3 输入文件格式，即数据来源 研究一下qmk，把轨迹球移动之后自动切换到第三层这件事给取消掉，再看看有什么可以配置玩的。 ON dating: run muscle in align and test mode via its online document. alignment is not much usefull on such huge dataset, why? or these gaps is normal on such taxa_num datasets? Filt them into small dataset will influent the pps_result? Can I get any acceptable pps_result? Edit TODO-list and mdbook-files for ensuring they are in correct format. Memo &amp; Comments #char_type: 367368 QiXi Festeval so that I did not much work today. Well, it’s weekend, just finish my workflow building tomorrow. 2024&#x2F;08&#x2F;12 (10.7h)Doing tasksDone tasks ‼ON dating: run muscle in align and test mode via its online document. (0.0h) ‼Edit TODO-list and mdbook-files for ensuring they are in correct format. (10.7h) Todo tasks in this week build my init dataset_list #fit_data rewrite align.sh #fit_test clear my basic rule of my model. #robustness_model Memo &amp; Comments#char_type : 403606;align.sh was wrong, I should align codon, not just nuc. maybe all align should be rerun, but i just need choose only one. it is not a big thing now. 2024&#x2F;08&#x2F;13 (8.6h)Doing tasksDone tasks ‼build a init dataset_list, maybe just hemiptera but i should make them into datasets, not library. #fit_data (6.3h) get acc from shit_csv files. #fit_data (2.3h) Todo tasks in this week rewrite align.sh #fit_test clear my basic rule of my model. #robustness_model ‼download data from acc #fit_data Memo &amp; Comments#char_type : 434840; 2024&#x2F;08&#x2F;14 (8.1h)Doing tasksDone tasks ‼download data from acc (4.0h) align them into datasets. (4.1h) Todo tasks in this week rewrite align.sh #fit_test clear my basic rule of my model. #robustness_model align mito_2012&#x2F;2019 cds #fit_data Memo &amp; Comments#char_type : 456049 ; mafft-xinsi for rrna and mafft-linsi for cds? #alignment 2024&#x2F;08&#x2F;15 (9.9h)Doing tasks align mito_2012&#x2F;2019 cds #fit_data (10.0h) Done tasks rewrite align.sh #fit_test (1.0h) check linsi&#x2F;ginsi&#x2F;einsi diff. (0.7h) run GTR rb_pps_data in current dataset. #fit_test #pps (8.2h) Todo tasks in this week ‼clear my basic rule of my model. #robustness_model current running: Dating: rb_pps. tower: mito_9&#x2F;2_annotate; check the result. Memo &amp; Comments#char_type : 486401; whole mito data cannot be split into genes simply. maybe I just need to test the whole align and the partial gene data. It’s not important so it will be ok. maybe I need to cut some column via seqkit or seqconverter because of gaps and maybe I can get better dataset. 2024&#x2F;08&#x2F;15 (8.7h)Doing tasksDone tasks ‼align mito_2012&#x2F;2019 cds #fit_data (0.0h) current running: Dating: rb_pps. tower: mito_9&#x2F;2_annotate; check the result. (2.7h) try to cut gap column after align(triml seqkit seqconverter). #fit_test #alignment (0.0h) ‼find the way of getting char&#x2F;vars in rb. #fit_test #pps #revbayes (0.0h) find the way to use super computer. (0.0h) rb_pps for genomic dataset, bact, ahe (0.0h) Todo tasks in this week ‼clear my basic rule of my model. #robustness_model understand the revbayes protocol current running: dating: bcod_rb_pps; tower: mito2019rna_align, bactCao_rbpps. check the result to be run: ahes_Cao_rbpps, mito2&#x2F;9_rb_pps; Memo &amp; Comments#char_type : 522966; list models, check every datasets. I need a table. Beside revbayes, I need some other tests, maybe. I need test the best model they used or best model under model-finder, not just GTR. Covarian and GHOST, how to test? maybe I should learn about how to enter model file into revbayes first, then think about my model. 2024&#x2F;08&#x2F;17 (11.0h)Doing tasksDone tasks current running: dating: bcod_rb_pps; tower: mito2019rna_align, bactCao_rbpps. check the result (1.4h) list models i should test and find out how to test&#x2F; input them into revbayes. #fit_test (9.6h) Todo tasks in this week ‼clear my basic rule of my model. #robustness_model understand the revbayes protocol to be run: ahes_Cao_rbpps, mito2&#x2F;9_rb_pps; I need a stand table for dataset&#x2F;model&#x2F;vars check. Memo &amp; Comments#char_type : 541852 2024&#x2F;08&#x2F;18 (8.2h)Doing tasksDone tasks understand the revbayes protocol (0.0h) which covarian-like model i need to test? Should I discuss the “test conflict” between model fit and heterogenety test? #fit_test (5.5h) how to sample from pps? (2.7h) Todo tasks in this week ‼clear my basic rule of my model. #robustness_model to be run: ahes_Cao_rbpps, mito2&#x2F;9_rb_pps; I need a stand table for dataset&#x2F;model&#x2F;vars check. how to get dataset from treebase? #fit_data Memo &amp; Comments#char_type : 570013 2024&#x2F;08&#x2F;19 (5.6h)Doing tasksDone tasks to be run: ahes_Cao_rbpps, mito2&#x2F;9_rb_pps; (1.9h) I need a stand table for dataset&#x2F;model&#x2F;vars check. (0.0h) rewrite auto.sh (3.7h) Todo tasks in this week clear my basic rule of my model. #robustness_model how to get dataset from treebase? #fit_data genrate genral pps protocol. not only revbayes. and make a change with the protocol in pp calculation (maybe parameter selection or model mixture) #fit_test Memo &amp; Comments#char_type: 593248; 2024&#x2F;08&#x2F;20 (7.6h)Doing tasksDone tasks generate a couse viewer for datasets. #fit_cause (7.6h) Todo tasks in this week clear my basic rule of my model. #robustness_model how to get dataset from treebase? #fit_data genrate genral pps protocol. not only revbayes. and make a change with the protocol in pp calculation (maybe parameter selection or model mixture) #fit_test Memo &amp; Comments#char_type : 614842 今天发现之前的p-value计算脚本出错了，sim和emp的比较错了一位，已经改正 理解了R做图的脚本，发现没啥用，不如直接用pvalue脚本 理解了pvalue脚本的输出，其中low和upper就是emp值在pp分布中的上和下的概率，但是这都包括了恰好等于的部分，所以mid是把恰好等于取一半的修正，然后效应量是计算“模拟行为多大程度改变了样本总体”，效应量越小越好，太大不好。 2024&#x2F;08&#x2F;30 (24.5h)Doing tasksDone tasks how to get dataset from treebase? #fit_data (0.0h) genrate genral pps protocol. not only revbayes. and make a change with the protocol in pp calculation (maybe parameter selection or model mixture) #fit_test (18.7h) why there are some ‘nan’ in eff-test? (5.8h) Todo tasks in this week clear my basic rule of my model. #robustness_model ‼generate phylobayes approach. #fit_test Memo &amp; Comments#char_type : 666424 2024&#x2F;08&#x2F;31 (10.2h)Doing tasksDone tasks ‼generate phylobayes approach. #fit_test (10.2h) current run: pb_ances_test in tower and ahe_rb_pps in dating(because of memery limit in tower). (0.0h) Todo tasks in this week clear my basic rule of my model. #robustness_model Memo &amp; Comments#char_type : 696500 2024&#x2F;09&#x2F;04 (18.2h)Doing tasksDone tasks clear my basic rule of my model. #robustness_model (0.1h) ‼what is next? how long will i run? how to compare? (11.7h) learn covarion model and matrix in revbayes (2.9h) learn about Q matrix in markov process (3.5h) Todo tasks in this week download data from treebase, generate a protocol. #fit_data ‼Simulate data in various rate&#x2F;composition parameters. #fit_data PhyloMAd? Is it generate a different test? well, actually I just need to calculate the posterior predictive distribution for now. If PhyloMAd just generate different statistics, I can study it later. Memo &amp; Comments#char_type : 769353 2024&#x2F;09&#x2F;05 (3.0h)Doing tasks using parallel for pb_cm2. (-12.7h) Done tasks learn about covarion model (3.0h) Todo tasks in this week download data from treebase, generate a protocol. #fit_data ‼Simulate data in various rate&#x2F;composition parameters. #fit_data PhyloMAd? Is it generate a different test? well, actually I just need to calculate the posterior predictive distribution for now. If PhyloMAd just generate different statistics, I can study it later. Memo &amp; Comments#char_type : 826647 2024&#x2F;09&#x2F;08 (23.8h)Doing tasksDone tasks using parallel for pb_cm2. (0.0h) learn about covarion model (0.0h) check the cov model file (23.8h) Todo tasks in this week download data from treebase, generate a protocol. #fit_data ‼Simulate data in various rate&#x2F;composition parameters. #fit_data PhyloMAd? Is it generate a different test? well, actually I just need to calculate the posterior predictive distribution for now. If PhyloMAd just generate different statistics, I can study it later. Memo &amp; Comments#char_type : 879369 2024&#x2F;09&#x2F;10 (13.7h)Doing tasksDone tasks build a mbl model file. (13.7h) 通过几个数据集计算phylobayes和revbayes的simulation是否有区别。后续全面转向phylobayes和其他语言吧，rev不靠谱。 (0.0h) Todo tasks in this week download data from treebase, generate a protocol. #fit_data ‼Simulate data in various rate&#x2F;composition parameters. #fit_data PhyloMAd? Is it generate a different test? well, actually I just need to calculate the posterior predictive distribution for now. If PhyloMAd just generate different statistics, I can study it later. 构建mmms和ghost的实现？或者找办法模拟一下。 Memo &amp; Comments#char_type : 907274 2024&#x2F;09&#x2F;11 (0.0h)Doing tasksDone tasksTodo tasks in this week download data from treebase, generate a protocol. #fit_data ‼Simulate data in various rate&#x2F;composition parameters. #fit_data PhyloMAd? Is it generate a different test? well, actually I just need to calculate the posterior predictive distribution for now. If PhyloMAd just generate different statistics, I can study it later. 构建mmms和ghost的实现？或者找办法模拟一下。 JC_mbl的文件还差一点，就是var输出的时候格式有一点不对，pps跑不起来，应该是branchlenth输出不应该是向量的问题吧，test.var3可以跑，查看一下原因。 Memo &amp; Comments#char_type : 929101 2024&#x2F;09&#x2F;17 (0.0h)Doing tasks ‼Simulate data in various rate&#x2F;composition parameters. #fit_data (-19.8h) Done tasks PhyloMAd? Is it generate a different test? well, actually I just need to calculate the posterior predictive distribution for now. If PhyloMAd just generate different statistics, I can study it later. (0.0h) 构建mmms和ghost的实现？或者找办法模拟一下。 (0.0h) JC_mbl的文件还差一点，就是var输出的时候格式有一点不对，pps跑不起来，应该是branchlenth输出不应该是向量的问题吧，test.var3可以跑，查看一下原因。 (0.0h) Todo tasks in this week download data from treebase, generate a protocol. #fit_data read lie markov model paper. Memo &amp; Comments#char_type : 968136 2024&#x2F;09&#x2F;19 (0.0h)Doing tasks ‼Simulate data in various rate&#x2F;composition parameters. #fit_data (-21.9h) read “alisim not good enough” paper (-22.4h) Done tasks read lie markov model paper. (0.0h) Todo tasks in this week download data from treebase, generate a protocol. #fit_data Memo &amp; Comments#char_type : 983605 2024&#x2F;09&#x2F;22 (0.0h)Doing tasks ‼Simulate data in various rate&#x2F;composition parameters. #fit_data (-12.9h) Done tasks read “alisim not good enough” paper (0.0h) Todo tasks in this week download data from treebase, generate a protocol. #fit_data read fbd paper and learn about it. Memo &amp; Comments#char_type : 992598 2024&#x2F;09&#x2F;24 (0.0h)Doing tasksDone tasksTodo tasks in this week download data from treebase, generate a protocol. #fit_data Simulate data in various rate&#x2F;composition parameters. #fit_data ‼read fbd paper and learn about it. Memo &amp; Comments#char_type : 1020221 2024&#x2F;09&#x2F;27 (0.0h)Doing tasks ‼read fbd paper and learn about it. (-13.9h) Done tasksTodo tasks in this week download data from treebase, generate a protocol. #fit_data Simulate data in various rate&#x2F;composition parameters. #fit_data Memo &amp; Comments#char_type : 1037327 2024&#x2F;09&#x2F;29 (0.0h)Doing tasksDone tasks ‼read fbd paper and learn about it. (0.0h) Todo tasks in this week download data from treebase, generate a protocol. #fit_data Simulate data in various rate&#x2F;composition parameters. #fit_data ‼learn about alisim in jc_cov or something(ghost?). #fit_data Memo &amp; Comments#char_type : 1043569 2024&#x2F;10&#x2F;07 (0.0h)Doing tasks ‼learn about alisim in jc_cov or something(ghost?). #fit_data (-13.9h) Done tasksTodo tasks in this week download data from treebase, generate a protocol. #fit_data Simulate data in various rate&#x2F;composition parameters. #fit_data edit bin&#x2F;simulator.sh in mac:workplace&#x2F;test&#x2F;. #fit_data Memo &amp; Comments#char_type : 1059768 2024&#x2F;10&#x2F;14 (4.4h)Doing tasksDone tasks download data from treebase, generate a protocol. #fit_data (0.0h) Simulate data in various rate&#x2F;composition parameters. #fit_data (0.0h) ‼learn about alisim in jc_cov or something(ghost?). #fit_data (4.4h) edit bin&#x2F;simulator.sh in mac:workplace&#x2F;test&#x2F;. #fit_data (0.0h) test if alisim is same as revbayes pps. #fit_data (0.0h) covarion model format translate via phylogears #fit_data (0.0h) Todo tasks in this week sync my simulator.sh into tower. #fit_data Memo &amp; Comments#char_type : 1104641 2024&#x2F;10&#x2F;16 (1.7h)Doing tasksDone tasks sync my simulator.sh into tower. #fit_data (1.0h) zhongqii files. (0.0h) Liang Zonglei`s bpp. (0.7h) learn about &lt;expected pps&gt; article. (0.0h) Todo tasks in this week R script learning #fit_cause Memo &amp; Comments#char_type : 1115319 2024&#x2F;10&#x2F;31 (12.6h)Doing tasksDone tasks R script learning #fit_cause (3.5h) HPC learn. (7.7h) combine data and inf into one script. (1.4h) parallel auto in hpc. (0.0h) test mpi time used (0.0h) test parallel and mpi time used (0.0h) Todo tasks in this week inferrence in R. #fit_cause learn about R analysis. error cascades pippeline. short seqs run under a suitable cores number. sims run experimental design. Memo &amp; Comments#char_type : 1381138 2024&#x2F;11&#x2F;06 (0.0h)Doing tasks build a donelist pipeline between hpc and tower (-13.6h) Done tasks short seqs run under a suitable cores number. (0.0h) storage usage in hpc report; (0.0h) archive xz in auto.sh; (0.0h) done file list check between hpc and tower (0.0h) old done output. xz and tar. (0.0h) Todo tasks in this week ‼inferrence in R. #fit_cause learn about R analysis. error cascades pippeline. ‼Based on F81 Model sets, design sims run experimental. read science causual infer paper. file translation; Memo &amp; Comments#char_type : 1540811 2024&#x2F;12&#x2F;01 (0.0h)Doing tasksDone tasks ‼inferrence in R. #fit_cause (0.0h) learn about R analysis. (0.0h) read science causual infer paper. (0.0h) file translation; (0.0h) build a donelist pipeline between hpc and tower (0.0h) read causal inference. (0.0h) learn about tree comparation. rf distance; quartet distance. alias. (0.0h) ‼make a plot for zhongqii ppt. (0.0h) change the color of heatmap_smooth (0.0h) edit the code of raw_summary.txt.fmt in R. (0.0h) fig 4 add motation. (0.0h) Todo tasks in this week error cascades pippeline. ‼Based on F81 Model sets, design sims run experimental. make a base matrix for analysis. my pps rev code. make topology prior be the same. test its influence in a small dataset. ‼read hohna paper 2023: model selection is no read yang ziheng paper : 2005 branch length prior influences. add convergence evaluation Memo &amp; Comments#char_type : 1879514 2024&#x2F;12&#x2F;13 (0.0h)Doing tasksDone tasksTodo tasks in this week error cascades pippeline. ‼Based on F81 Model sets, design sims run experimental. make a base matrix for analysis. my pps rev code. make topology prior be the same. test its influence in a small dataset. ‼read hohna paper 2023: model selection is no read yang ziheng paper : 2005 branch length prior influences. add convergence evaluation change ppt $problem to a frame. year summary. make a plan for next-step working. Memo &amp; Comments#char_type : 2015115 2024&#x2F;12&#x2F;15 (0.0h)Doing tasksDone tasks make a base matrix for analysis. (0.0h) change ppt $problem to a frame. (0.0h) year summary. make a plan for next-step working. (0.0h) learn about argc (0.0h) make ai server env (0.0h) Todo tasks in this week error cascades pippeline. ‼Based on F81 Model sets, design sims run experimental. my pps rev code. make topology prior be the same. test its influence in a small dataset. ‼read hohna paper 2023: model selection is no read yang ziheng paper : 2005 branch length prior influences. add convergence evaluation Memo &amp; Comments#char_type : 2063913 2024&#x2F;12&#x2F;16 (0.0h)Doing tasksDone tasksTodo tasks in this week “build your own group”. send a email to Prof. Luo Arong year-end summary slides for group willing. ‼integrate aichat and todo and my hexo blog together Memo &amp; Comments#char_type : 2085460 #ai_shell:你的工作包括文件和目录操作（如复制、移动）、配置文件编辑、脚本编写与调试、以及使用aichat等工具处理任务管理、信息查询等相关事务。 #aichat:这两天的工作内容基本没有变化，包括向罗教授发送邮件、为意愿小组制作年终总结幻灯片以及将AI聊天工具、待办事项和Hexo博客整合在一起。 2024&#x2F;12&#x2F;18 (0.0h)Doing tasksDone tasks ‼integrate aichat and todo and my hexo blog together (0.0h) revbayes in rstudio and rmarkdown for beamer (0.0h) Todo tasks in this week year-end summary slides for group will. statistics cor re-check clear the purpose that contacting Prof. Luo With the purposes, make a list that i need to do before sending email Memo &amp; Comments#char_type : 2115970 #ai_shell:你的工作涉及查看和处理Shell历史记录、编辑待办事项列表、使用aichat进行代码相关任务，以及运行和管理iqtree命令来分析基因序列。 此处iqtree主要是阅读文献PhyloForge，关于“SV Signal-Based Population Phylogeny”部分的复现。主要评价如下： 我翻了一下文中的代码，这里所说的sv系统发育，实际上是用indel的01矩阵跑iqtree和model finder…是以前处理形态矩阵时比较基本的一个处理，然而iqtree做树重建时没有考虑zipfian分布之类的indel模型，也不会因为二态矩阵而做相关处理，或许此处的创新可能在于对矩阵的编码方案与之前有所不同…然而依我拙见，sv在系统发育基因组的应用局限并不在重编码10矩阵上，否则可以通过“令xx为0，xx为1”的方式将任何事物变成“系统发育信号”…从这个角度上来讲，这篇文章提供的方案并不算是“基于sv的系统发育重建”，至少文中没有可见的论证…indel数据被应用到系统发育研究中已有很多解决方案，但似乎一直因有效性而难以推广，因而在基因组级别上并不常见，按此文说法或许这篇文章是第一次。数据扩大到基因组级后，有效性问题被数据淹没，增加采样频率确实可以降低信噪比，但考虑到相比系统发育研究的问题，样本总是寡而有偏的，这种信号的提高所带来的帮助是有局限和瓶颈的…况且，相比过采样，增加信号强度和更好的滤镜似乎对最终结果的影响是更大的。总之，注意到近年来系统发育基因组领域对sv数据的热情关注，我想良好的重建方法会在不久的将来出现。 #aichat:这两天的工作变化集中在完成了将AI聊天、待办事项与Hexo博客集成的任务，同时在年终总结幻灯片和统计检查方面有所进展，并开始整理与教授罗联系的目的及其所需准备的事项。 2024&#x2F;12&#x2F;26 (0.0h)Doing tasksDone tasks download CLASSIC datasets for testing. (0.0h) use GTR alias as example, apply the Tame prior and moves. (0.0h) Todo tasks in this week year-end summary slides for group will. statistics cor re-check ‼clear the purpose that contacting Prof. Luo With the purposes, make a list that i need to do before sending email read totally the Molecular Evolution by Ziheng Yang 2014. write a header comment block generator in rust the alpha in gamma 4 category prior: in a simple dataset, uniform(0,10^8) is better, but how about a heterogenity dataset? study on it. Memo &amp; Comments#char_type : 2207749 今天仔细看了Tame先验和四足动物线粒体2024sb两篇文章的代码，以及revbayes的教程和源码文件，确认了先验、参数和moves的设置。使用gtr进行了标准化，其中branch lenth部分有比较大的更改，一些moves也有调整，应该重跑一些看看效果。 除此之外，branch rate可以在之后的time calibrating中设置，这是分区&#x2F;全局的速率参数，用来计算绝对枝长的。目前我的分析应该用不到，但是以后其他研究中，不需要考虑是否必须在一次mcmc中全部采样的问题，branch lenth可以同时，也可以之后进行计算。之后计算：https://revbayes.github.io/tutorials/sequential_bayes/stepwise_dating同时计算：https://revbayes.github.io/tutorials/clocks/2025&#x2F;01&#x2F;02 (0.0h)Doing tasksDone tasks year-end summary slides for group will. (0.0h) clear the purpose that contacting Prof. Luo (0.0h) With the purposes, make a list that i need to do before sending email (0.0h) read totally the Molecular Evolution by Ziheng Yang 2014. (0.0h) all models file. (0.0h) change the ‘model_name’ part. (0.0h) Todo tasks in this week statistics cor re-check write a header comment block generator in rust the alpha in gamma 4 category prior: in a simple dataset, uniform(0,10^8) is better, but how about a heterogenity dataset? study on it. hpc&#x2F; dating&#x2F; tower : mcmc genertation, tsv2nex etc; change it in auto.sh, not one by one. heterotachy test in old papers. check them and calculate them. Memo &amp; Comments#ai_shell: #aichat:","categories":[{"name":"diaries","slug":"diaries","permalink":"http://zhangdeweb.site/categories/diaries/"},{"name":"TODO","slug":"diaries/TODO","permalink":"http://zhangdeweb.site/categories/diaries/TODO/"}],"tags":[{"name":"daily","slug":"daily","permalink":"http://zhangdeweb.site/tags/daily/"},{"name":"work","slug":"work","permalink":"http://zhangdeweb.site/tags/work/"}]},{"title":"2024年终总结","slug":"2024summary","date":"2024-12-27T16:00:00.000Z","updated":"2025-01-01T16:12:13.096Z","comments":true,"path":"2024/12/28/2024summary/","link":"","permalink":"http://zhangdeweb.site/2024/12/28/2024summary/","excerpt":"","text":"刚刚参加完中期考核，才后知后觉的发现博士生涯已经算是过半了。可惜眼下完成的工作很难说是进度过半，但无论如何，一年到头，还是应该为自己的工作作简单总结，这不仅为了自我反思，更是希望自己可以梳理头绪，明晰下一步的计划安排，并判断是否要适当调整自己的长期目标。那么，首先回顾一下今年都发生了什么吧。 回头来看，坦率地讲，今年是从一个颓废、混乱的情绪中开始的。去年十月恍恍惚惚地开了一个马马虎虎的题，如今看来也是非常冒险的行为，对分析方案的很多细节，其实当时的认知是非常模糊的。加之开题时对课题的讲述可能也不够清楚，老师们给出了应该构建一个新模型的建议，导致在开题之后的一段时间里，除开一些简单的软件编译、安装，我主要沉浸在对构建模型毫无头绪的情绪中。如今看来，这是一种闭门造车，并不可取。首先，我其实在开题之前就思考过这个问题了，对异速性是否要重新建模，能否建出可用模型，如何建模，这些问题都应该基于对当前模型的误差分析结果，这也是我开题的时候选择误差分析的最主要因素。在没有误差分析结果之前就去盲目建模，如今看来，想必会是没有头绪的。可惜当时的我头脑并不清晰，对自己课题的认识也不够深刻、坚定，加之我本人也容易产生焦虑情绪，多重因素下，耽误了很多时间。这样的状态从开题之后一直持续到春天，期间除了阅读一些文献，思考一些有的没的的问题，主要只完成了神农架标本的分发与大叶蝉亚科的鉴定，一些基础分析软件的安装与调试。由于这种空洞的思考没有带来任何实际的进展，枯燥的我于春天去四周放松一下心情，但不幸遇到了一个不大不小的车祸，又耽误了一些时间，如此便来到了6月份。 转机发生在应我导师要求参加“第十九届昆虫分类区系学术研讨会”。其实对该会议我也早有耳闻，但早先时候一门心思埋在自己的世界里，也觉得自己没有任何成果，并没有意愿参加什么交流，在群里征集志愿者时，我便没有吭声。然而当时大家都在外出采集，留校的人也不多，也不知是否是导师察觉到我的低沉，希望我和外界有所沟通，总之最后导师直接点名要求我作为会议志愿者，不知是无心插柳还是有意为之。这次会议又见到去年神农架采集、南京培训所认识的很多老师、同学，尤其是南京农业大学的张峰老师，对我颇有鼓励，可是当老师同学们关心我的课题情况，我非常惭愧，并没有能说出什么所以然。在这种鼓励与压力之下，我才猛然醒悟到之前的时间是因为我的消沉和迷茫而浪费掉了。迷茫和呻吟根本不是我所希望看到的自己的样子，振作起来，勇敢面对，即使失败了也没什么大不了。会议的晚宴上，我导师带着师姐们社交，我也蹭了过去，导师在社交时把我们介绍给领域里的大小同行，在一个间歇，他指着我的背说，把胸挺起来。我也不知道该怎么形容当时的感觉。在之后实验室的小聚餐里，我头脑一热，和导师作出这样的宣言：不发一篇好文章我不走。如今想来，这是冲动的，并不合时宜，但当时的我认为如果不这样在公开场合作出如此表示，我怕自己又会如之前一样消极迷茫，如此表态，给足自己压力和动力，尽力去做事，就算失败了也没什么的。 之后我重新整理课题思路，认识到了之前的认知错误，才真正意义上开始了我课题的工作。可惜这时候已经来到了七月份，上半年到头来对后续工作有意义除了那些环境部署、软件安装，我看的文献在后续的工作里也幸好能有所作用。七月开始做数据预处理的流程，八月开始重视流程标准化，到这主要构建好了pps相关的流程框架。后面主要遇到两个问题，一个是我的小服务器无法满足计算需要，另一个问题是pps之后的分析无从下手。9月份主要学习了化石生灭过程做树先验或模拟的内容，了解了李马尔可夫模型，然后就是调整优化计算代码，处理一些遇到的问题，简单做了并行处理。小服务器的问题在十月下旬一次组会后，导师安排了学校的计算平台，基本满足计算需求，并针对计算平台调整了代码，做了文件压缩，多端同步等处理。十一月读了结构因果的书，对数据推理和分析有了进一步的认识，对pps结果的解读和挖掘有了大概的思路，同时，十一月通知今年我需要进行中期考核，但目前的结果没有整理成图，因而又花了一些时间学习R语言绘图。图做差不多之后，十二月初用beamer做了汇报所需的slides。十二月十日中期汇报，提前三天过ppt，十号汇报的时候老师们也没说什么，主要建议后期结合生物现实讨论问题，然后建议我去外面多和别人交流交流。老师们似乎都很重视交流，我确实也认识到自己需要多交流，回来与导师汇报了中期的情况，导师说我应该自己先联系，并表示如果我有需要会帮我联系联系。 今年的流水账就是这样了。我发现一个人闭门造车的时候，很容易钻牛角尖，由于一般这种时候很难有所突破，此时工作就会陷入低谷。幸运的是我在两次低谷期都遇到了不得不进行的交流，重点倒不是我能在交流中收获什么神迹般的指引，而是我在交流之前必须对自己的工作重新熟悉、总结、归纳，往往在这个过程中，我会形成新的思考和认识，并对解决当前困境有一定的帮助；同时在交流中获得一些正面评价也很能鼓舞人的士气。有了干劲，似乎什么都没那么难了。或许以后我应该养成定期主动总结自己工作的习惯，似乎是有些好处的，不能算浪费时间。 总的感受是：今年虽然一开始经历了一点小曲折，但我的工作总算是正式步入正轨，并迅速的全面铺开；同时，得益于之前“浪费”的时间，我对一些基本的代码知识还算了解，这也使得在下半年能快速的摸索到一些结果，在中期考核中不算太丢人现眼。今年导师的那两句话令我印象非常深刻，不知道我导原意是什么，总之在我的理解中是很提振我的精气神的话，而且都是在我非常需要这种话的时候他讲出来的。最后，今年我自己的一大感悟就是，做科研一定要学会取悦自己，或者说要会自我激励。以前的我，很容易精神内耗，经常站在他人视角评判自己，最后做事就会畏手畏脚，并且对身边人的变化也很敏感，这样实在是无意义的，而且很拖累我做事的效率。后来认真剖析了一下自己，才发现我专注在自己喜欢的事情的时候，其实是很轻松愉快的，而且我也不会因为结果的好坏而影响我去做这件事，换句话说我可以接受自己的失败，再换句话说，无论他人眼里的我多失败，我也有信心处理好自己的事，享受自己的生活，这是我的原生家庭，我从小所受教育带给我的积极的一面，想通这里，我从此便专注于自己的事情，不再盲目的精神内耗。以前的我总是活在远虑近忧里，而接连克服了一些困难，我相信最终我会解决好当前的科学问题的，而无论它是什么形式——无论是什么文章，无论是什么结果，无论我在哪里，我会因为好奇心而去做这些事。可以说目前为止，我掌握一些基本的代码技巧，也对自然现象有一定的认识了解，对生物演化有自己的一点浅薄的哲学认知，并正在试图将它们汇总成我的博士论文，同时，对于AI的使用与协作，我也有粗浅的涉猎，早些时间训练过图像分类模型，现在也部署了自己的代码辅助语言模型。最终应该也不会落得没有饭吃。有饭吃，有事做，生活充实，人生惬意，如此甚好。 过去的一年有所磕绊，希望来年的工作可以更加顺利顺心。 首先，正如中期汇报中总结的，pps的计算已然不成问题，但是仍需补充一些现实数据，这个工作放在日常里逐步完成吧。值得注意的是，文本摘要技术或许是可用的，可以节省一些时间。 因果推断的理论我大概了解了，应该再去看一些具体的软件包，甚至是代码源码。我想我应该掌握自己构建mcmc的方法，并且考虑一下哪些地方是不是需要自己从头写。 回顾一下todolist里留存的待办事项，有一些事情耽搁好久了，该着手去办了。 长远来看，应该把工作规整一下，看看怎么写小文章的问题。今年肯定是没戏了，下次一定。 把目前的工作再细化一下，然后再总结一下，看看到底有什么问题，去和其他老师交流交流。这几件事情，从紧要程度来分，做的顺序应该是3，5，2，4，1。 明年定一个小目标吧，找大佬交流一次，然后至少把前两部分工作细化、做完。做完的意思是能到发表级别，而不是只自己看着挺好就行了的。目标不要定太高，工作要慢慢做，把它做好，做细，做精，做到自己满意，而不求太快。最后希望自己在2025年能享受知识，心无旁骛，保持好奇，保持热爱，保持愤怒；祝愿自己的探索最终可以成为有令自己满意的结果。 来年见！","categories":[{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://zhangdeweb.site/tags/blog/"}]},{"title":"自我批判与批判反思","slug":"thinking4","date":"2024-11-27T16:00:00.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2024/11/28/thinking4/","link":"","permalink":"http://zhangdeweb.site/2024/11/28/thinking4/","excerpt":"","text":"经常想到这样的批判：解决的不是实际&#x2F;具体的问题。诸如“没有类群”之类的批判，可以认为来自总结式思维，即从具体的事例开始总结知识，而不是从逻辑推演开始。 经验是好事，要先有经验，再去学习，很多时候这是符合直觉的。但忽略逻辑性无疑是走在科学的反方向上，任何过度依靠经验的领域最终都成为了艺术，而过度依赖经验的伪科学最终只能走向神秘主义，比如占星术等，成为部分信徒眼中无与伦比的至高真理。某种程度上讲，虽然经验推动了科学发展，但科学实际上是在对抗经验依赖的。 分类学，系统发育研究，或者说生物历史的研究，如果说是科学的，那只能是数据驱动的科学。在很长的一段时间里，复杂的生物运动无法被很好的描述，经验占据了这类学科的主导，见多识广的学者可以轻易得到一些很容易推广的可靠结论，而经验不足的研究者无法对这些结论进行检验。可是如今的科学技术发展已经足以对一些复杂的系统进行一定程度的描述了，单纯依据经验得到的结论很难再具有说服力。 一个基本的科学素养，是论点和论据能够很好的结合在一起。可是很多研究，测量了很多数据，做出一个结论，但是并没有说明二者的联系。这就好像在说“我看到太阳从东边升起，所以显然1+1&#x3D;2”。如果这样的结论没有太多争议，那还没有什么问题，可是系统发育研究的结论往往充斥着不一致，没有仔细论证的论据自然经不起推敲。更甚之，还有一些时候，数据拒绝了研究者的相关结论，但研究者有意或无意地忽略了。 在基于数据的研究中，有两件事是同样重要的，即数据和对数据的解释。如果没有很好的解释，那数据只是一些符号，没有用处；而如果脱离数据，那整个学科就会完全倒向对经验的依赖，最终走向神秘主义，成为伪科学。我当然也要强调经验的重要性，但经验的作用是在为研究提供潜在的、研究之外的信息，比如即便没有测量，我们也可以先验的注意到一些生物习性，这些习性可能会影响后续的分析结果，而不是将经验用来当作结论的。 当我想用基因组数据说明某个类群的生物具有相同共同祖先时，我能获得多大程度的支持？或者说我为什么能得到这样的结论，而不是相反的结论？在实际研究中说明这一点其实并不是难事，但是在我们领域里往往不这样做，这种现象的后果就是高比例的结论不可重复。 回到一开始的问题，对于那些批评：为什么不去解决一个类群里的具体问题？我的回答是，我不认为我们目前可以解决一个类群的具体问题，或者说目前能解决的问题已经解决了，剩下的是目前没有能力解决的问题了，再换句话说，我不认为现在有哪个类群的具体问题可以通过测序解决。任何问题最终都要服务到一个具体问题上，但这个过程，这个工作，不一定非要由我来做。虽然我确实有兴趣去完成它，但当前更紧要的，我需要先处理的事项，则是背后的逻辑问题。基于某一个类群讲故事，很难把这样的工作做好，不是合适的研究方案，也不符合我的思维习惯。 最后，我想明确的强调，目前生物学没有发现任何普适性规律，而这背后的含义就是当今生物学没有任何结论是足够可靠的，在这样的背景下，“具体类群”的分析，只具有提供数据的价值。无论如何，数据是重要的，而结论的可靠性同样重要，且在当今数据极大丰富的环境下更为急迫。","categories":[{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://zhangdeweb.site/tags/blog/"}]},{"title":"关于paralogs建树和基因家族分析的粗浅了解","slug":"thinking3","date":"2024-11-16T16:00:00.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2024/11/17/thinking3/","link":"","permalink":"http://zhangdeweb.site/2024/11/17/thinking3/","excerpt":"","text":"首先，系统发育研究里用ASTRAL-PRO可以实现基因家族树-物种树的估计，这个过程里实际上给出了祖先所拥有的基因数目的概率信息。构建基因家族树也可以直观看到基因是否发生了复制&#x2F;丢失事件。 比较基因组分析里的CAFE分析，把基因家族数目当作特征，利用生灭过程给出了节点状态的概率分布，重建节点状态。但是这里没有考虑基因树信息。 如果有了一个具体的基因树，自然是不需要CAFE来做基因家族扩张收缩的分析的，看看那些分子生物学结合的研究就可以知道。而在组学分析里是不需要具体的基因树的，所以直接将其当作数量性状做重建就可以了，如此可以节省大量的基因树计算的时间。 基因家族动态应该是很敏感的，稀疏的节点状态是很难准确估计的，而且功能-基因分析里，祖先状态并不是一个必要信息，而是作为一种矫正。这里可以引用辛普森悖论。考虑这样一种情况，某基因里抗性的AT高，敏感的AT低，但支系之间AT含量不同，且部分支系由于长期的演化已经产生了适应，表现出补偿效应，有一个较低AT的支系已经表现出综合抗性的，并占据新的生态位，并由此产生了辐射进化，多样性极大丰富。如此一来，在统计抗感性状和AT含量时，就会发现大部分物种AT低则有抗性，AT高则抗性不强，得到与事实相反的结论。而在现实研究里，补偿效应可以通过组学分析直接得到，很难讲系统发育历史对表型有什么直接的、跨越基因相似性的影响，更多的是作为补充证据。 科学是基于经验总结规律然后不断验证，但是实际上科学是对抗经验的，亦或是科学的目的是摆脱经验的依赖，这甚至可以是科学的定义，并指导了我们的一切。我在学习的时候必然要学习经验、形成经验，但是最终，我要摆脱经验，如此才是一个完整的工作。 补充一些待探索的名词：reconciliation（这是一种和msc类似的解决基因树-物种树冲突的过程），orthovenn软件自带了基因家族扩张收缩的树表示（而非基于生灭过程的）。 这个有时间看看能不能把流程整合一下。","categories":[{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://zhangdeweb.site/tags/blog/"}]},{"title":"远离让自己不舒服的地方，走入舒适圈","slug":"thinking2","date":"2024-09-26T16:00:00.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2024/09/27/thinking2/","link":"","permalink":"http://zhangdeweb.site/2024/09/27/thinking2/","excerpt":"","text":"天下熙攘，皆为利往；好好科研，不如赚钱。叮叮当当，开写文章；查分区表，按顺序找。这个也行，那个也好；只要能中，就不算瞎搞。一图一表，一台电脑；反复润色，仔细观瞧；故事精妙，水平很高，拿出来看，是和尚洗澡。路边听闻人闲聊，说养牛不能吃奶挤草。 （和尚洗澡——毛都没有） 投机主义者们有温床，是我不该待在这里让自己不舒服。 “当你觉得身边的人全是傻逼的时候，你才是这群人里的傻逼！” 是的，但我这个傻逼也不想改变自己现在的样子，所以我应该找一些和我一样的傻逼聚在一起，这样我就不会觉得身边的人全是傻逼了，如此这样，就消除了问题。","categories":[{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://zhangdeweb.site/tags/blog/"}]},{"title":"近日文献检查有感","slug":"thinking","date":"2024-09-18T16:00:00.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2024/09/19/thinking/","link":"","permalink":"http://zhangdeweb.site/2024/09/19/thinking/","excerpt":"","text":"老生常谈了：树形民主不是好文明，“多数人暴政”恐成新误差。 有一些工作测了大量的数据，然后把市面上能见到的、自己电脑&#x2F;服务器能跑的分析全跑一遍，然后把这些结果拿到一起，选一个喜欢的、顺眼的，指着它说：“20次分析里18次都能得到这种结果，它就是受到强烈支持的，而其他结果就不适合我们的这个独特而迷人的类群！” 在这样的研究中我很难感受到科研工作的严谨性，取而代之的是一种“民主”而又“自由”的精神。可惜这样的精神很难在这种时候感染到我，我只能祝福他们那如同咕噜的“precious”的“类群”确实有着足够多的、其他人难以名状的“故事”。","categories":[{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://zhangdeweb.site/tags/blog/"}]},{"title":"WISH-list","slug":"WISH_LIST","date":"2024-08-09T16:00:00.000Z","updated":"2025-01-01T16:12:13.096Z","comments":true,"path":"2024/08/10/WISH_LIST/","link":"","permalink":"http://zhangdeweb.site/2024/08/10/WISH_LIST/","excerpt":"","text":"财务收入： 每年积攒1w，多余部分扣除50%固定储蓄，除了固定储蓄外其余的钱用来实现愿望 积蓄可用期限为1年，超过一年的积蓄转变为固定储蓄 固定储蓄不可用于愿望清单内容 按绩效降低固定储蓄扣除比，绩效考核标准每年更新 每年6月1日为财务周期更新的起始日期，规定旅游补贴、绩效指标等事务，并清理账目 2024.6.1: 10000元+0元固定储蓄，日均补贴333元 2024.8.21，合川、都江堰之旅，使用5天额度 小憨八硕士期间有个舒适的工作环境 屏幕2024.8 屏幕对比度 屏幕颜色校准 主机 鼠标2023.12 键盘2024.6 vpn和网络配置2024.8 手机vpn 手机谷歌商店 服务器 发一篇sci 工作日里可以和小憨八多多联系 多邻国好友2024.8 远程机器人 挖掘平常一起玩的游戏 COC （已经玩腻）2023.12 金铲铲 王者荣耀 调理大憨八健康 肠镜检查 挂号 住宿 有一个属于我们自己的小窝 存钱 1 2 5 10 20 30 50 100 选一个好城市 选一个好位置 选一套好装修 一起走遍天涯海角——旅游打卡 小憨八成长的地方 大憨八成长的地方 山东（吃北方海鲜） 海南 三亚（吃南方海鲜） 北京 山西 天津 河北 河南 辽宁 吉林 黑龙江 江苏 浙江 安徽 江西 湖北 湖南 广东 四川 成都 熊猫基地2024.2 杜甫草堂2024.2 太古里2024.2 宽窄巷子2024.2 都江堰2024.8 青城山2024.8 贵州 云南 陕西 西安2024.6 太白2024.6 陕北 陕南 甘肃 青海 台湾 内蒙古 新疆 乌鲁木齐 市区 新疆农业大学门口2023.9 新疆博物馆2023.9 西藏 宁夏 广西 上海 重庆 市区2024.6 合川2024.8 海外 大憨八的数码产品 换一个小屏，信号好，续航长，系统流畅，有耳机孔，最好系统开源易于定制的手机 一台喜欢拿在手上玩的相机 一台兼顾生态摄影和综合素质的相机 镜头：200mm f2 黑白胶片 相机 镜头 komura 100 2.5 显影系统 放大系统 装裱设备 无人机2024.6 vlog相机2023.6 hifi2024.7 耳机 大耳 放大器 小尾巴 云存储 100T 内存条达到300G 100 完全属于我的键盘 了解一下qmk，把轨迹球移动之后自动切换到第三层这件事给取消掉，再看看有什么可以配置玩的 Something u jwe need to do 一起见大憨八父母 一起见小憨八父母 大憨八父母见小憨八父母 相见让思念有了意义 小憨八研一 1 2024.6 2 2024.8 3 2024.8 4 2024.10 5 2024.11.11 6 2024.12.10 7 8 9 10 11 12 小憨八研二 1 2 3 4 5 6 7 8 9 10 11 12 小憨八研三 1 2 3 4 5 6 7 8 9 10 11 毕业快乐！","categories":[{"name":"TODO","slug":"TODO","permalink":"http://zhangdeweb.site/categories/TODO/"}],"tags":[{"name":"Wish-list","slug":"Wish-list","permalink":"http://zhangdeweb.site/tags/Wish-list/"}]},{"title":"TO-DO LIST [Update Daily]","slug":"00-TODO-LIST","date":"2024-08-07T16:00:00.000Z","updated":"2025-01-01T16:12:13.092Z","comments":true,"path":"2024/08/08/00-TODO-LIST/","link":"","permalink":"http://zhangdeweb.site/2024/08/08/00-TODO-LIST/","excerpt":"","text":"# Usage ## Planning 1. TODO-list is for work and WISH-list is for life. 2. Add tasks should be done recently into autodaily and finish them in order. ## Documenting 3. Done the works on the autodaily everyday and summary daily work at autodaily-report. 4. Add tags for tasks in autodaily for linking to items in TODO/WISH-list. ## Work progressing 6. Sketch my thesis on mdbook. (because markdown is easier for both documentation and presentation) 7. Sync the sketches to TODO-list and break it up into exactly tasks. 8. Report my work into mdbook weekly. ## Presentation 9. Use mdbook (for short report), beamer or PPT (for formal report) to show contents in mdbook. My hobbies Migrate my BLOG from hexo into zola. Hugging rust. My skills and curios My daily life My duty","categories":[{"name":"TODO","slug":"TODO","permalink":"http://zhangdeweb.site/categories/TODO/"}],"tags":[{"name":"TODO","slug":"TODO","permalink":"http://zhangdeweb.site/tags/TODO/"}]},{"title":"Filed work in 2023","slug":"diary/2023","date":"2023-12-30T16:00:00.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2023/12/31/diary/2023/","link":"","permalink":"http://zhangdeweb.site/2023/12/31/diary/2023/","excerpt":"","text":"title: Finally about to get my own research topictags: - daily work - skelevisioncategories: - diariesdate: 04-07-2023 What I have done today Installed 4090 GPU on tower server and configured its driver. Reproduced the skelevison paper with their data and preliminary attempted with my own data. It is still hard to say I can make it work for me, but it does present a initial results which is that each part can be identified under man-made annotations when I use the scripts provided on github. Reported the second point mentioned above to my supervisor. What I need to do next Install the only left 8-pin power cord for the GPU. Done Read literature, evaluate the feasibility of this method. Write my dissertation report. Given a new research using a similar approach to study phylogeny, my original idea may be feasible. However, I need to find out a new innovation point for my own project. Specifically, I need deal with trait definition and quantification, modeling the evolution process against this data type, evaluate trait quality in evolutionary analyse, combine multimodal data like pictures and molecular sequences. Since this is a PhD project, I need to assess whether I can achieve enough results to meet my graduation requirements. Namely I need to plan to publish 3 articles at least. What I want to do in future Continue configure my edit environment in archlinux with old laptop. Done Summarize installation and configuration of archlinux. What I want to tell myselfRead papers first. Find out the key issue. Clarify my research ideas and logic.It should be excited for me that finally about to get my own research topic and I need to think over and plan it very carefully. title: A normal day with something forgottentags: - daily workcategories: - diariesdate: 04-08-2023What I have done today Install 4090 GPU power cord. Did some housework. Installed hexo blog environment and something else in old laptop. Organize all pictures from Dr. Liang Zonglei and process them with train.py and predict.py. What I need to do next Add a curtain to my tower chassis. Add a GPU physical support. Done Sting out the margin. Study with the annotation and learn how to definition traits like them. Add internal links among blog articles. Done What I want to do in future Write down archlinux blog environment installing process. Including fcitx, xorg, i3wm, obsidian, vial with linux install, clash for windows with tun mod in linux, zsh, zim, p10k, exa, neovim with ^M problem, vim-plug, and others. Deal with the problem that firefox web browser cannot visit google. Done What I want to tell myselfToday is Saturday. Be busy tomorrow. title: Sunday, I have to be busy tomorrow.tags: - daily work - git - vimcategories: - diariesdate: 04-09-2023What I have done today Added the GPU support. Configured zimfw and zsh-z for tower. Configured Lightdm for tower followed by https://blog.csdn.net/caoshiying/article/details/107242980. .gz files need a gzip tool by command gzip -c &lt;*&gt;.gz &gt; &lt;output_file&gt;. What I need to do next Nothing should be continued by today’s work. What I want to do in future Works left before. What I want to tell myselfGit is a useful tool when dealing with complex objects. I have used it several times but I don’t understand its basic logic clearly. Today, I re-learned Git and document my notes. git add add files to track. A new file created in workspace needs this command to be added in git’s track and controlled by git. Changed files controlled by git in the workspace also need this command to be added in storage. git commit sent files in storage to local repository. git push push local commits to remote repository. git log check commit histories. Use -h followed by the command to check more info. git reset back to specified state. git revert revert existing commits. git rm remove files in storage. git pull sync workspace to the remote repository. If workspace have been changed after latest version from remote repository, move or remove local files.That’s it right now. Others more complicated will be learned when I meet it. Another tips I have learned today is that how to add or cancel comments in VIM. ctrl + q to enter visual block mode. use “hjkl” or other command keys to select range to be changed. shift + i to enter insert mode. Add comment symbol on the first line. Press esc to quit insert mode and the range selected will be added symbols typed before.This method can be used to many situations. If you want to cancel the comments, press x to delete comment symbols after selecting range. title: A day with fails and conda.tas: - daily work - i3wm - blog - 2022secategories: - diariesdate: 04-10-2023What I have done today Added internal links among my daily blogs. More info Something about attachments in blog: Configure hexo and use source file folder. Follow this. .md files always appear on the homepage wherever it locates in sources folder or not. .txt will present contents on the web page. Files without .* will be download to local. Use [](*.*) to cite files in source. .* is always suggested to be added. Obsidian is not a good tools to deal with attachments. It does not show files which is not followed by .md. Maybe configuring neovim more carefully is a good idea. Configured Aur source for yay in archlinux running on my old laptop. In this way, the problem of firefox has been solved by the way, which can be replaced by configures of chrome. Just do yay -S google-chrome and follow this. Tried to reproduced the 2022SE used scripts from github. But my conda cannot install all dependency and stopped at train step. Maybe the configures of sources of conda is wrong. The reason why I reproduced this article is that I want to try if the recognition with non-man-made annotations can split pictures and how much it refines the outlines. What I need to do next Structures outline definition. Annotation changing. Traits definition. Maybe same thing to the first point. What I want to do in future Archlinux configure process. Conda configuration. Hexo or other blogs custom configure. What I want to tell myselfShortcuts is a basic operation of daily use but I cannot use or custom them easily before, especially when I used Vial to custom my ergo keyboard. Today I get a new useful tool, xev, from here to confirm my key coding includes values of ‘keysym’ and ‘keycode’. Now I can configure my own shortcuts in i3wm with command bindsym for ‘keysym’ and bindcode for ‘keycode’ in ~/.config/i3/config. And here is my config file. title: I don’t want to depend with the dependencytags: - daily work - conda - 2022secategories: - diariesdate: 04-11-2023What I have done today Eliminated the version incompatibility problem between CUDA and PyTorch. When I tried to run train processes in reproduction to 2022SE, occurred erros: 1[/home/junru/anaconda3/envs/rsfin/lib/python3.7/site-packages/torch/cuda/__init__.py:104](https://file+.vscode-resource.vscode-cdn.net/home/junru/anaconda3/envs/rsfin/lib/python3.7/site-packages/torch/cuda/__init__.py:104): UserWarning: NVIDIA GeForce RTX 4090 with CUDA capability sm_89 is not compatible with the current PyTorch installation. The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37. If you want to use the NVIDIA GeForce RTX 4090 GPU with PyTorch, please check the instructions at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) CUDA version is a new one so I need to upgrade my torch version. But when I use the conda environment with python==3.7, the default pytorch version will be 1.7.0. So It seems that the problem occurred somewhere else. The latest version of pytorch supporting python==3.7 is pytorch 1.13.0,See here and here. So update with conda then. Added voice player to Goldendict. Install mpv for translate-shell by sudo apt install mpv. See Audio Option in man trans for more. Add audio program in GoldenDict. Edit-Dictionaries-Programs-Add Enable&#x3D;yes; Type&#x3D;Audio; Name&#x3D;audio; trans -speak %GDWORD%; Additionally, the commands for plain text is: Trans to English as trans -no-ansi -e google -s auto -t en-US -show-original y -show-original-phonetics n -show-translation y -show-translation-phonetics y -show-prompt-message y -show-languages n -show-original-dictionary y -show-dictionary y -show-alternatives y “%GDWORD%” Trans to Chinese as trans -e google -s auto -t zh-CN -show-original y -show-original-phonetics n -show-translation y -no-ansi -show-translation-phonetics n -show-prompt-message y -show-languages n -show-original-dictionary n -show-dictionary y -show-alternatives y “%GDWORD%” What I need to do next Update pytorch.Done Continue the reproduce.Done title: Day for outdoorstags: daily work blogcategories: diariesdate: 04-12-2023 What I have done today Went to LouGuanTai in ZhouZhi, Xian. Caught cicadas and play around. Fat flister beetle and Cicadas had been caught Tips: The method of add pictures into hexo blogs, see here. If the format and notes of pictures need to set, it is have to use html grammar, see more . Pictures make websites start slowly, so use it carefully. Here is a simple example: 12345678910&lt;div align=center&gt;&lt;img src=&quot;/Pictures/*.jpeg&quot; width=&quot;500&quot; /&gt;&lt;br&gt; &lt;div style=&quot;color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;&quot;&gt; Add Title here&lt;/div&gt;&lt;/div&gt; IOS application Working Copy can edit files and sync with github. So that I can edit blogs on my iphone and deploy it using ssh remotely. title: Goodbye, rubbish conda!tags: - daily work - mamba - 2022secategories: - diariesdate: 04-13-2023What I have done today The previous error report of my reproducing CUDNN_STATUS_EXECUTION_FAILED maybe caused by the imcorrespond of version among cuda, cudnn and pytorch see and see. Updating dependency with conda makes me crazy. It only runs at an extremely slow speed and I cannot change to a source makes me satisfy. I tried to uninstall all of conda and reinstall it but it won’t help after trying for several times. I tried to install mamba followed by guide. After installed mamba, I installed conda environment with mamba create env -f conda_environment.yml after removing several packages’ version info or it cannot be found in current sources. Dataset preparation runs well but train still have errors. AttributeError: module ‘torchvision.models’ has no attribute ‘mobilenet_v2’ It caused by trochvision version is too low, see. So do mamba update torchvision AttributeError: module ‘torch.jit’ has no attribute ‘unused’ It caused by corresponding problem between pytorch and torchvision, see. So do mamba install torchvision==0.14.0 -c nvidia -c pytorch and install failed. Caused by threads number limited, see. Use ulimit -n 2048 and torchvision will be update correctly. Check mamba list | grep torch and this will be shown: 1234567WARNING conda.core.prefix_data:_load_site_packages(291): Problem reading non-conda package record at lib/python3.7/site-packages/certifi-2020.6.20-py3.7.egg-info/PKG-INFO. Please verify that you still need this, and if so, that this is still installed correctly. Reinstalling this package may help. _pytorch_select 0.1 cpu_0 anaconda pytorch 1.13.1 py3.7_cuda11.7_cudnn8.5.0_0 pytorch pytorch-cpu 1.1.0 py37he1b5a44_0 conda-forge pytorch-cuda 11.7 h778d358_3 pytorch pytorch-mutex 1.0 cuda pytorch torchvision 0.14.1 py37_cu117 pytorch And torch version in python had been 1.1.0. What I need to do next Change pytorch version to 1.13.0 and run train script.Done What I want to do in future Learn about version corresponding or dependency among cuda, python or so on. What I want to tell myselfCalm down you clever. Just a dependency problem. Solve it quickly and move on! title: It works!tags: - daily work - 2022secategories: - diariesdate: 04-14-2023What I have done today Ran the training script with a little change. Most errors I met before caused by version corresponding among torch, torchvision, cuda, cudnn. Especially, remove _pytorch.select package in conda to use pytorch with gpu. Recreate environment of ‘rsfin’ followed by: 12conda install pytorch==1.13.0 torchvision==0.14.0 torchaudio==0.13.0 pytorch-cuda=11.7 -c pytorch -c nvidia # to solve problem with some models not found in torchvision. - mamba update ffmpeg # to solve problem with libopenh264.so.5 not found Runs and errors occurred in line 23 torchvision.transforms.RandomRotation(20, resample=Image.BILINEAR): 1TypeError: __init__() got an unexpected keyword argument &#x27;resample&#x27; This caused by torchvision version changing. See help(torchvision.transforms.RandomRotation) in python and report: 1234567891011121314151617class RandomRotation(torch.nn.modules.module.Module) | RandomRotation(degrees, interpolation=&lt;InterpolationMode.NEAREST: &#x27;nearest&#x27;&gt;, expand=False, center=None, fill=0) | | Rotate the image by angle. | If the image is torch Tensor, it is expected | to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions. | | Args: | degrees (sequence or number): Range of degrees to select from. | If degrees is a number instead of sequence like (min, max), the range of degrees | will be (-degrees, +degrees). | interpolation (InterpolationMode): Desired interpolation enum defined by | :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. | If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. | For backward compatibility integer values (e.g. ``PIL.Image[.Resampling].NEAREST``) are still accepted, | but deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum. ... ... Interpolation and resample are two kinds of method in image processing. The difference between them is that interpolation is one way to realize resampling for separate data, see. And the difference between NEAREST and BILINEAR is that BILINEAR is more delicate, see. So change the code to torchvision.transforms.RandomRotation(20, interpolation=Image.BILINEAR) and all will be fine. Tried to run model test and error occurred in line 13 and 16. Variable config.TESTING_MODEL_WEIGHTS.items() do not have parameter folder_name and mtype. Went to the jobs fair and listened to the reports of Shaanxi insect society. 4 articles in CAS(Chinese Academy of Science) 1 &#x3D; 35w&#x2F;year 4 articles in CAS 2 &#x3D; 25w&#x2F;year Few choice. Prof. Cao yanghui suggests that do not merge morphological traits dataset into molecular datasets because of the quantity difference and the unclear weighting rules. What I need to do next Solve the weights value problem. Read codes and it is easy.Done What I want to do in future Systematic learn deep learning programming. What I want to tell myselfThe basic problem in the whole ‘version error’ is that my GPU do not support torch version 1.5 and conda cannot install all dependency they listed. So I have to change some packages in the dependency. The newer package I use have some change in grammar. Luckily, I found out where the problem is and solved it at last. title: Why there are three best?tags: - daily work - 2022secategories: - diariesdate: 04-15-2023What I have done today Checked the errors. Changed config.py because I had reorganize data. (Or there would be errors same to editing with a wrong folder name.) 12# TESTING_DATASET_NAME = &#x27;test_data&#x27;TESTING_DATASET_NAME = &#x27;datasets/test_data&#x27; And returned path error: FileNotFoundError: [Errno 2] File /mnt/data/trait/reference/2022se/insects-recognition-main/datasets/datasets/test_data/df_img_meta.csvdoes not exist. There are two datasets directories in the path. Tried to use absolute path in both DATASETS_LIST and TESTING_DATASET_NAME in config.py and the path error solved. Significantly, model_test.py needs directory weights locate at object-home-path and using relative path in config.py. New error occurred in test_model function with AssertionError: .test_model was import from model_test.py and error occurred at line 180 which is caused by assert function. Error message: 1234567891011121314151617 --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) in 18 print(config.TESTING_DATASET_NAME) 19 test_model(model_name=config.TESTING_MODEL_NAME, dataset_name=config.TESTING_DATASET_NAME,---&gt; 20 folder_to_evaluate=folder_name, model_type=mtype) 21 if config.TESTING_INTERPRETABLE_PLOTS: 22 make_interpretable_plots(model_name=config.TESTING_MODEL_NAME, /mnt/data/trait/reference/2022se/insects-recognition-main/src/models_test.py in test_model(model_name, dataset_name, folder_to_evaluate, model_type) 178 # Формирование табличного отчета по качеству классфикации 179 # Для объективной оценки испольузем только лучшую эпоху по валидации. ---&gt; 180 assert len(reports_list) == 1 181 report_dict = reports_list[0] 182 df_detailed_report = pd.DataFrame([]) AssertionError: The comments in line 178 and 179 said that we need a best something, namely the length of reprots_list must be 1 or occurred this error. Check variable reports_list in models_test.py. 1234567891011EVAL_DICT = &#123;&#x27;device&#x27;: DEVICE,&#x27;weights_folder&#x27;: weights_folder,&#x27;test_loader&#x27;: test_data_loader,&#x27;model&#x27;: nnet,&#x27;criterion&#x27;: torch.nn.CrossEntropyLoss(),&#x27;target_names&#x27;: list(TRAIN_DATASET_KWARGS[&#x27;class_dict&#x27;].keys()),&#x27;class_dict&#x27;: CLASS_DICT,&#x27;data_hash&#x27;: train_data_hash,&#x27;external_data_class_dict&#x27;: None,reports_list = model_evaluate(EVAL_DICT) Check function model_evaluate in ./utils/model_test_utils.py. 1234567891011121314151617def model_evaluate(evaluate_dict):weights_folder = evaluate_dict[&#x27;weights_folder&#x27;]weights_list = [os.path.join(weights_folder, x) for x in os.listdir(weights_folder) if &#x27;.pth&#x27; in x]epoch_dict = dict([(int(x.split(&#x27;.&#x27;)[0].split(&#x27;_&#x27;)[-1]), x) for x in weights_list])sorted_epochs = sorted(epoch_dict)reports_list = []for epoch in tqdm.tqdm(sorted_epochs): weights_path = epoch_dict[epoch] print(&#x27;Epoch: &#x27;, epoch) print(&#x27;Weights path: &#x27;, weights_path) if evaluate_dict[&#x27;external_data_class_dict&#x27;]: report_dict = eval_epoch_on_external_data(weights_path, evaluate_dict) else: report_dict = eval_epoch(weights_path, evaluate_dict) report_dict.update(&#123;&#x27;epoch&#x27;: epoch&#125;) reports_list.append(report_dict)return reports_list Check the length of reports_list in for by adding print(len(reports_list)) and found that the length grew up to 3 from 1 caused by reports_list.append(report_dict). What I need to do next Find out how append works and why use it. How to make it correct. title: Trying to make plots.tags: - daily work - 2022secategories: - diariesdate: 04-16-2023 What I have done today An easiest way to solve reports_list length error is annotate the assert functions. So I annotate assert functions limiting reports_list (and weights_list) length in test_model (and make_interpretable_plots) function on model_test.py line 180 (and 602). New problem is that there are only two empty result folders which should be three folders with result plots. The function creating result folders is make_interpretable_plots defining in model_test.py. But I do not find which statement creating the folder. I checked the variables and weights_list are 4 because there are four files in directory of weights_folder. weights_path is ok because it is set to weights_list[0] by the authors. I checked the next variable occurred, manual_images, defined as a parameter in function make_interpretable_plots. It set to a list of pictures named by SHA. This parameter was set manually in the model_test.ipynb which the pictures I do not have. So I annotated the contents in the variable manual_images and I got the testing for my own testing datasets. This process may be last for half an hour to several hours. Previous trying make mistakes in manual_images of make_interpretable_plots function so that it did not running correctly. Now it runs completely and results plots are shown. What I need to do next When the process is finished, keep checking this error. Up to now, there are a few questions about this error in my mind. The assert function limit something must be only one and they are not. What is meaning of statements followed by asserting manual_images is a list and why. Which function create folders in results? How to evaluate the identification and make it useful for me? Report my work to my tutor. title: High-resolution screen with larger fonts.tags: - daily work - 2022se - i3wm - roficategories: - diariesdate: 04-17-2023What I have done today Previous application launcher process, dmenu, is not really good for me. It is difficultly to configure so I decide to turn to rofi, a modern application launcher and windows switcher to replace dmenu. Installation with sudo apt install rofi and remove dmenu. Download themes for rofi from here. More about usage and rofi config. Followed by the documents, move theme files to ~/.config/rofi/themes and preview themes by running rofi -modi run -show run and select rofi-theme-selector. Set default theme by editing rofi config file, which created by command rofi -dump-config &gt; ~/.config/rofi/config.rasi, to add @theme &quot;MyTheme&quot; in the end following configuration &#123;&#125;. Use rofi -help to check configuration file path. Fonts and other settings can be edit in theme files. Rofi has several built-in modes implementing common use cases. Here is the differences of commonly used ones. run: launch applications from $PATH, with option to launch in terminal. drun: launch applications based on desktop files. It tries to be compliant to the XDG standard. window: Switch between windows on an EWMH compatible window manager. ssh: Connect to a remote host via ssh. filebrowser: A basic file-browser for opening files. Configure i3wm to change application launcher. 1234567# annotate dmenu command# bindsym $mod+d exec --no-startup-id dmenu_run -fn DejaVu-25 # A more modern dmenu replacement is rofi:bindsym $mod+d exec --no-startup-id &quot;rofi -modi drun,run -show drun &quot;bindsym $mod+w exec --no-startup-id &quot;rofi -modi drun,run,window -show window # remember to remove conflicting shortcuts Modes drun launch applications based on desktop files. The way to add application to the menu is that add .desktop file in directory /usr/share/applications/. An easy way for adding is copy another file and edit the appropriate lines for name, exec, and icon. It is recommended to use full path. It seems like relate to xdg, so may be man xdg-open, man xdg-settings can tell more. Training models with my own pictures dataset from Dr. Liang Zonglei. (I used full pictures include several structures in this time.) Edit dataset path and re-organize files in dataset folder. Remember: DO NOT only use number as file name or as the start of file name, or error occur with function len() during training. Prepare dataset and train. All will be fine. When train is done, change TESTING_MODEL_WEIGHTS to current folder name. Testing datasets and new error occurred. 1234567891011121314151617181920212223242526272829303132RuntimeError Traceback (most recent call last) in 18 print(config.TESTING_DATASET_NAME) 19 test_model(model_name=config.TESTING_MODEL_NAME, dataset_name=config.TESTING_DATASET_NAME, ---&gt; 20 folder_to_evaluate=folder_name, model_type=mtype) 21 if config.TESTING_INTERPRETABLE_PLOTS:22 make_interpretable_plots(model_name=config.TESTING_MODEL_NAME, /mnt/data/trait/data/test_se/src/models_test.py in test_model(model_name, dataset_name, folder_to_evaluate, model_type) 174 &#x27;external_data_class_dict&#x27;: None, 175 &#125; --&gt; 176 reports_list = model_evaluate(EVAL_DICT) 177 178 # Формирование табличного отчета по качеству классфикации /mnt/data/trait/data/test_se/src/utils/model_test_utils.py in model_evaluate(evaluate_dict) 163 report_dict = eval_epoch_on_external_data(weights_path, evaluate_dict) 164 else: --&gt; 165 report_dict = eval_epoch(weights_path, evaluate_dict) 166 print(&#x27;hahahahahahahahahahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh&#x27;) 167 print(reports_list)/mnt/data/trait/data/test_se/src/utils/model_test_utils.py in eval_epoch(weights_path, evaluate_dict, plot_roc_curve) 51 model.cuda() 52 ---&gt; 53 model.load_state_dict(torch.load(weights_path)) 54 model.eval() 55 y_true, y_pred = [], [] ~/mambaforge-pypy3/envs/rsfin/lib/python3.7/site-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict) 1666 if len(error_msgs) &gt; 0: 1667 raise RuntimeError(&#x27;Error(s) in loading state_dict for &#123;&#125;:\\n\\t&#123;&#125;&#x27;.format( -&gt; 1668 self.__class__.__name__, &quot;\\n\\t&quot;.join(error_msgs))) 1669 return _IncompatibleKeys(missing_keys, unexpected_keys) 1670 RuntimeError: Error(s) in loading state_dict for MobileNetV2: size mismatch for classifier.1.weight: copying a param with shape torch.Size([32, 1280]) from checkpoint, the shape in current model is torch.Size([5, 1280]). size mismatch for classifier.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([5]). What I need to do next Keep debugging. Done What I want to tell myselfToday I find that the thing I did recently is debugging for codes. It does not have much fun to me. When I have done with it, I should find out a scientific question and focus on natural history. title: Running complete. Now, explain it!tags: - daily work - 2022secategories: - diariesdate: 04-18-2023 What I have done today The error occurred yesterday caused by using an incomplete model weight file, see. Re-trained model and used new directory path. Running completed. The identify result is not pretty good. Because I did not organize these pictures in class carefully and I used the full picture including different structures. This time, I tried to use the results of 04.07 which includes pictures of separate structures. Copy result folders to the datasets directory and rename it to ‘raw’. Remove files that do not need. Organize files using a shell-script. Start running that series of programs and get results. What I need to do next Read the article again and evaluate the results.Done title: Ready for group meeting?tags: - daily work - 2022se - vim - rofi - unicodecategories: - diariesdate: 04-19-2023 What I have done today Tried to scale up font size and windows in i3wm followed this and found that it is not easy. Evaluated the results, shown in the article. Dataset was split into three consequent sub- samples: training, validation and testing – 70%, 15% and 15%.… …To measure a model quality, several metrics were used: averaged by class and weighted by a value of class instances types of precision, recall, F-score and top-1 error rate. The precision is a ratio of true- positive classification and the sum of true-positive and false-positive classification. The recall is a ratio of true-positive classification and the sum of true-positive and false-negative classification. The F-score is a harmonic mean of precision and recall metrics.… …For LIME we set ‘top_labels’ to 1 to produce explanations only for the class with the highest confidence value, ‘num_features’ to 5 to limit the number of features that are presented in an explana- tion. The green areas are treated in favour of the true class, and the red ones are treated as the regions against the true class. For Grad- CAM, we choose the last layer visualization because it is a layer that accumulates more primitive features that are extracted from previous layers. For RISE the number of masks ‘N’, the size of the smaller square binary masks ‘s’ and the probability of each pixel in the smaller masks ‘p’ were empirically set to 1000, 8 and 0.2, respectively.… …LIME and Grad-CAM algorithms were much more likely to show the expected areas of interest within which the recognized object was located. Some of the visualization results obtained with RISE focused in great detail on the image of the area within the specimen. Namely, I should check those value and LIME results. Top-1 error rate about means mismatch rate, see. Make PPT for group meeting tomorrow. What I want to tell myself For those application complied locally which had been installed in */bin/, the source file can be deleted. For inserting digraph or unicode symbols in neovim, follow here. Type CTRL-Q in insert mode to enter visual mode and type U or u following unicodes and then type enter or space to quit the mode and the symbol will be typed in. Additional, the rofi can help type unicode symbols by install rofimoji and set shortcut in i3wm title: En attendant Godottags: - daily workcategories: - diariesdate: 04-20-2023What I have done today Waited for group meeting and meeting had been canceled. Maked PPT. What I need to do next Read articles and prepare for PhD object. title: Upgrade WiFi connection to AP modetags: - daily work - latex - zotero - csl - routercategories: - diariesdate: 04-21-2023What I have done today Installed Tex Live for unix but there is no package management good enough for ubuntu. tlmgr cannot be found in sudo, see. Installed [zotero] for inserting citations in .docx files. The method of custom reference format can be found here, more about CSL and a visualize tool. Installed a new WiFi router with AP mod but it cannot start ap mod successfully. So I still use dhcp. Additionally, the gateway cannot be same with ip in LAN masknet, remember to change one. Ready for writing. What I need to do next Search articles totally. title: Using WiFi router in ap mode with R6Stags: - daily work - router - latex - mountcategories: - diariesdate: 04-22-2023What I have done today Configured router with ap mode but no remote connection for its best performance. Use TpLink AX3000 XDR3010 which support ap mode to connect to R6S. R6S has a powerful CPU and TpLink has a higher speed than the previous wireless router, Oray X5. Compared with routing mode, ap mode make all devices under a same local network without creating a new subnet, see. Otherwise, it may improve performance by reducing forward times and letting the more powerful CPU of R6S deal with events. Since then, compared with previous one, the new scheme can improve Wi-FI speed and CPU efficiency in only one LAN. Ap mode of TpLink Wi-Fi router can be open after setting network setting - wired - ipv4 - shared to other computer in LAN network controller of R6S. Configure DHCP for R6S, followed here and a Chinese guide and another one. Fixed IP address should NOT be in DHCP and remember to cancel the annotation of authoritative. But it still cannot check which ip has been used and manage them in an easy way. Maybe there is a application can help me. Because of IP address changing, I need to reset my NAS auto-mounting by sudo vim /etc/fstab and sudo mount -a. Installed Oray pgyvpn in docker but because of using subnet it is not better than connecting Oray X5. IPV6 is not a good choice, either. Solved the problem of Latex package manager tlmgr cannot be found in sudo, followed by here. Run sudo visudo and add the /bin path into Defaults secure_path. What I want to do in future Connect Oray X5. Try to find out how to manage IPs in R6S.ubuntu. Done The connection in safari of iPhone is very slow. Find out the reason. Done Learn more about the basic of network technology for a better using of R6S. Install fonts from MacOS and Windows to ubuntu for Latex. title: Professional things should be left to professionals.tags: - daily work - router - openwrt - dns - LANcategories: - diariesdate: 04-23-2023What I have done today Problem of Safari network connection had been solved by changing DNS service. It should not be 8.8.8.8 or 114.114.114.114. Deleted options related in DHCP configuration which in /etc/dhcp/dhcpd.conf. Additionally, DNS configure file can be found here. Edit /etc/systemd/resolved.conf, run systemctl restart systemd-resolved.service and add link by sudo ln -s /run/systemd/resolve/resolv.conf /etc/. Installed Friendlywrt, a variant of openwrt on R6S, to instead of Friendlyelec-ubuntu. Considering the specialization and complexity of configuration, the operating system for router is better than Ubuntu. Especially, the setting of dhcp or slaac assignation of ipv6 address in ubuntu needs configure network interfaces (more info see man interfaces) but the default network configuration is using network-manager. Changing interfaces config file will cover settings in network-manager and I do not know how to combine them. The interfaces cannot be configured for ap router easily for me and the Ubuntu system takes more spaces and ram. Besides, I do not know how to check which ips I have used in Ubuntu. Considering I may have more complex needs in the future, compared to edit every thing from zero, I prefer to use configure-completed and powerful openwrt. Friendlywrt operating system installation is very easy by following their wiki. Use tower for network sharing to R6S in order to install python and pip by opkg update &amp;&amp; opkg install python3 &amp;&amp; wget &lt;get-pip3.py&gt; &amp;&amp; python get-pip3.py and then use pip install srun-cli to install the campus network auto-authentication program. Maintain the LAN topology and tplink wireless router sets to ap mode. Reset nas auto mount in tower.ubuntu. What I need to do next Set ip address, ipv6 and other settings in Friendlywrt. Be familiar with openwrt&#x2F;Friendlywrt system. Done Check the previous todo list and add tags for them. Done The speed of network is not slow. Maybe the speed of campus network is erratic. title: Hello cool ipv6 and goodbye.tags: - daily work - ipv6 - ssh - routercategories: - diariesdate: 04-24-2023What I have done today Set another IP for R6S.openwrt.LAN by following their wiki. SSH Pub-key authentication. It is more safety and convenient because it does not need a password every time to log in. Use ssh-keygen &amp;&amp; ssh-copy-id &lt;remote server ip&gt; to add pub keys into server. Edit local ~/.ssh/config file, to set a common name, with: 123Host &lt;server name&gt; HostName &lt;server ip&gt; User &lt;user name&gt; If pub key authentication cannot success, try to config file /etc/ssh/ssh_config and set StrictModes option to “no”, more about see man sshd and man ssh_config. Ipv6 configuration in R6S with openwrt followed by here. Sometime R6S cannot get an ipv6 address from campus net, maybe because of a network error. Reconnect the net cords and wait for a minutes, it will be fine. After the whole configuring, I can visit ipv6 sites in my tower device (connected with R6S by wired LAN). But it does not work on mac which is connected with R6S by wireless LAN. Besides, I cannot visit my ipv6 address outside my LAN because of the nat6. The reason of using nat6 is I can get only one address, not a sub-net, from campus network so I have to use nat6 to forwarding address to other devices. It's so cool! But it is sadly that I cannot visit my ipv6 address via WAN. What I need to do next Organize my to-do list for my PhD object.Done What I want to do in future Solved the remote access issue. Something may help here, such as nat64 in openwrt, and others, but it needs more settings to complete. What I want to tell myselfI think it is time to give up tossing these messes for now. I can just stop ipv6 and use a dumb methods, using Oray X5, because what i need is remote access, not ipv6. Otherwise, configure the network setting cost my too many time. I have lots of other more important business to do. title: Clipboard is a fantastic thing.tags: - daily work - polybar - rofi - clipboard - srun-clicategories: - diariesdate: 04-25-2023What I have done today Organized to-do list. I have tried to add a to-do list plugin on desktop environment, by using polybar module to run alacritty -e vim &lt;todo.md&gt;. I added it by learning the other modules added in polybar. I am using the polybar with theme forest which is from polybar-themes package in github. Polybar starts in i3wm and the startup file, launch.sh, can be found in ~/.config/polybar/launch.sh. 12345678910111213141516#!/usr/bin/env bashdir=&quot;$HOME/.config/polybar&quot;themes=(`ls --hide=&quot;launch.sh&quot; $dir`)launch_bar() &#123; ... ... polybar -q main -c &quot;$dir/$style/config.ini&quot; &amp; ... ...&#125;if [[ &quot;$1&quot; == &quot;--material&quot; ]]; then style=&quot;material&quot; launch_bar... ... Following the script, I check the file ~/.config/polybar/forest/config.ini because this is the theme I am using. The additional modules are included in user_modules.ini and added by variable modules-left/center/right. Add module todo in user_modules.ini by copying and editing the module/launcher. 12345[module/todo]type = custom/textcontent = ✔content-foreground = $&#123;color.yellow&#125;click-left = alacritty -e vim ~/blog/curiosusJR.github.io/source/_posts/00-TODO-LIST.md Add todo module into config.ini by changing it to: modules-center=date sep todo Restart i3wm. Installed an clipboard helper, greenclip, as a plugin of rofi. Added it into i3wm keyboard shortcuts. After installing the static binary file into ~/.local/bin, add execution permission with chmod +x and add auto-startup process in i3&#x2F;config with adding a line of exec_always ~/.local/bin/greenclip daemon. Remember that do not use --no-startup-id. Add shortcuts of auto-paste (see here) in i3&#x2F;config. bindsym Ctrl+v exec--no-startup-id rofi -modi &quot;clipboard:greenclipprint&quot; -show clipboard -run-command &#39;&#123;cmd&#125;&#39; &amp;&amp; sleep 0.2 &amp;&amp; xdotool type $(xclip -o -selection clipboard) In the way showed above, the space cannot be paste successfully. It can be solved by here, just add &quot; beside $(xclip -o -selection clipboard). The paste action cannot be canceled by default. It had been discussed here but all of them are not perfect. So I still use methods above and copy a space anytime, thus when I cancel a copy and it will only paste a space there. The most useful method (but still with bugs) seems like using a script create in ~/.local/bin/paste-modi.sh 123greenclip print $@coproc (xdotool key --clearmodifiers &quot;ctrl+v&quot; &amp;)exit 0 and run it with rofi -modi &quot;paste:/home/junru/.local/bin/paste-modi.sh&quot; -show paste Network broken retry. Because campus network login status will be refreshed every 24 hours or by some unknown reasons, I need to re-login when it logouts. Under the campus network, command ping can always success whether it has been login or not, so I have to use curl to get http code for checking network connection. Here is more info about using curl to get http codes and here is more info about http codes. In order to eliminate the impact of websites, I want to test a few sites. So here is the way to go through the groups. There are many campus network login tools on github or other repositories, such as. By sum up, create auto-login script as: 12345678910111213141516#! /bin/bashhttp_list=( www.baidu.com www.nwafu.edu.cn );http_code=404;error_times=0;for i in $&#123;http_list[@]&#125;do http_code=`curl -o /dev/null -s -w %&#123;http_code&#125; $i`; if [[ $&#123;http_code%??&#125; != 2 &amp;&amp; $&#123;http_code%??&#125; != 3 ]] then ((error_times=error_times+1)); fidone;if [ $error_times != 0 ]then srun-cli ;fi Auto-run the script with crontab in ubuntu or openwrt. 123456$ crontab -e#crontab* * * * * sh path/to/auto-relogin$ /etc/init.d/cron restart What I want to do in future It would be even better if the to-do list could pop up in a drop-down menu. I can create an object in github. Referring rofi source code and reading more documents will help me. The paste action cannot be canceled by default. It had been discussed here but all of them are not perfect. title: I remember that I have 2 ethernet card.tags: - daily work - ipv6categories: - diariesdate: 04-26-2023What I have done today Improved the network-broken-retry-script. Something about reading from files and checking numbers of lines in file. The websites for checking network connection are included in /root/.Network_Connection_Test_List. Log file will be written in /root/ when all the test sites are inaccessible and re-try for login. Using systemd.network to config ethernet card by checking man systemd.network. What I want to do in future Enable ipv4 in enp3s0 and disable ipv4 in enp0s31f6. Disable ipv6 in enp3s0 and enable ipv6 in enp0s31f6. title: The final work was done on the eve of holiday.tags: - daily work - ipv6 - ddnscategories: - diariesdate: 04-27-2023What I have done today Completed ipv6 setting on R6S, followed here. Use socat followed here for visiting LAN from ipv6-WAN. (This might be nat64.) It must be turned off the VPN or ip will be changed. At the last, set a firewall in openwrt. Ipv6 WAN connection from TCP&#x2F;UDP will be forwarded to selected port in selected LAN host (refer other default setting). Now, I can visit local servers using [ipv6]:&lt;port&gt; address. The ipv6 address can be changed when I restart my network server or reboot. So I need a DDNS service and a new domain name. Buy one from aliyun and set dns&#x2F;ddns services with DDNS scripts found at GitHub. Use crontab for automatic updating. Add a new testing user on tower-server for others to link to later by sudo adduser. What I want to tell myselfA few days recently, I focused on my LAN and its remote connection. It takes some time for solving various issues I did not familiar with before. The remaining im-perfections are my tower server security. I do not know if current solution is secure enough. But I think it is securely enough because of campus networks and I do not have many valuable things. It is really time to do something serious. I must devote all my energy to my PhD proposal, when I come back from vacation! 😜 title: Ready for my report.tags: - daily work - Ph.D - proposalcategories: - diariesdate: 05-04-2023What I have done today Preliminary organized my thinking of the proposal report. What I want to tell myselfI have come back from my vocation. Write my reports as draft in markdown. In my zsh configure, the git-information is shown by p10k plugin in zim. It can be found that the signs meaning in their github page. Git stash is a temporary action, more about. In following period of time, my work will be focused on writing my proposal report. For the integrity of the idea, my daily work will be concentrated in the draft of the report, rather than the separate log. title: KataGo and sabaki on towertags: - daily work - Go - katago - sabakicategories: - diariesdate: 05-16-2023What I have done today Installed sabaki, a desktop app for go game. Install the .AppImage file from its github and create a desktop file in /usr/local/applications/ for it. Installl the latest version from github. I had installed the TensorRT version followed: https://github.com/lightvector/KataGo#opencl-vs-cuda-vs-tensorrt-vs-eigen. (zip installation will report errors that tensorrt version mismatch. Build from source will solve this issue, see ) So I need to install tensorrt 8.5 using .deb package first followed: https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-853/quick-start-guide/index.html#installing-debian. Remember to install the CUDA 11.8 for this which means change the link of /usr/local/cuda. Use stat /usr/local/cudd to check which version it links to. Use sudo apt-cache madison tensorrt to check the installable version and select the correct one to install. If it cannot install its depends, trying to install depends on /var/nv-tensorrt-local-repo-*/*.dev. After installing tensorrt, reboot. Build from source, followed by here. Installation locate at ~/.opt/katago. Install the larger (40 block network) network model from https://github.com/lightvector/KataGo/releases/tag/v1.4.5. Usage. Add katago gtp -model &lt;&gt; -config &lt;&gt; in sabaki. Solved the issue that it gets stuck when copy contents from alacritty to chrome by re-build alacritty and use the whole directory to startup alacritty. Moved it to /opt/alacritty and link it to /usr/local/bin/alacritty. Added fonts from macos and windows11 in ubuntu. Copy font files .ttf to /usr/local/share/fonts/custom and set code 744 in chmod. And then sudo mkfontscale &amp;&amp; sudo mkfontdir &amp;&amp; sudo fc-cache -fv. title: Insect phylogenetic analysis using image clustering methodtags: - daily work - phylogeny - image clustering - reproducification - kailancategories: - diariesdate: 05-22-2023What I have done today Reproduced the work of Tao and Sun (2022) using datasets from 2022SE. Download the python 3.8.5 script in supplementary data. The directory should be organized in the way below: 1234567.├── dataset│ ├── ...├── Learn_pld_376_Supplement 3. Image Cluster Score_mmc3.py├── Learn_pld_376_Supplement_2_Image_Clustering_mmc2.py├── train└── validation Install the dependence. Use conda for python 3.8.5 installation. Use pip for tensorflow installation. Change the path and modelSpList for your data. Change the num_classes for your own data. For checking the image file is broken or not, use ffmpeg to change the format of images in your dataset. The script I have edited for my using and adding some annotation can be found here . When the clustering done, the directory should be added some files like this: 123456789101112.├── dataset│ ├── ...├── fine-tune-inceptionResnetV2N.h5├── fine-tune-resnet50NN.h5├── Learn_pld_376_Supplement 3. Image Cluster Score_mmc3.py├── Learn_pld_376_Supplement_2_Image_Clustering_mmc2.py├── randomTrees38.nexus├── train│ ├── ...└── validation ├── ... The file named randomTrees38.nexus is 1000 random distance-based trees using inceptionRestnetV2 model. The number and model can be changed in the python script easily after reading it. Calculate the consensus tree using Paup 4.0 , which has a detailed manual. Simply put, the script of Paup below can calculate the consensus tree. Use help &lt;command&gt; in Paup for more info. 12Execute ./randomTree38.nexuscontree /majrule=yes percent=50 LE50=yes treefile=consensus_1000_inceptionResnetV2.nexus; What I need to do next Change number and models for testing and retry. Check the plots of epoch accuracy for the model. Check the topology of consensus tree and the SSI value using the script in Supplement 3. Output the trait matrix if possible. title: Trait extraction and output in image clustering.tags: - daily work - image clustering - kailan - trait extraction - continuous trait matrixcategories: - diariesdate: 05-23-2023What I have done today The dataset I used was wrong. It only contain the outgroups so I re-downloaded dataset with zenodo_get from https://zenodo.org/record/5866217#.ZGxPMdJBwUE. Changed batch_size=30, epochs=500 in model settings, voteTimes=500 in distance calculation, treeNum=10000 in random tree generation and re-run the script. Consensus tree was calculated in the same way to yesterday. Checked the results in Figtree and found that species from same genus did not get together and the results of two runs with different paramerters were significantly different. I should use whole dataset to try again and use different parameters to find out the if it will convergence to a situation or not. Image features were extracted by function get_image_feature in supplementary file 2 with Keras. There is a global variable picDic recorded all features of pictures randomly selected in all species in get_image_feature function. After running the whole script, I can output picDic to a .csv file for getting the raw feature data. The feature data is a 1 d array contain normalized RGB information, which means it does not contain location correlation information. This part can be further optimized. Output picDic to .csv used the script below: 123for i in picDic.keys(): f=open(&quot;../output/&quot;+i.split(&#x27;/&#x27;)[1]+&quot;.csv&quot;, &quot;a&quot;, encoding=&quot;utf-8&quot;) np.savetxt(&quot;./output/out_&quot;+i+&quot;.csv&quot;, picDic[i], fmt=&#x27;%f&#x27;, delimiter=&#x27;,&#x27;, newline=&#x27; &#x27; ,encoding=&#x27;utf-8&#x27;) And then I can get 813 files which is the length of picDic and every file contain 1536 continuous features. What I need to do next Use whole dataset with different settings and model to conduct trees and check the topology. Current (newer one) parameters shows a worse result in the curve plots. Try more. RevBayes and Beast can deal with continuous traits and mrbayes cannot. Try to build trees in a bayes way. Official suggesting that: 1Deprecated: tf.keras.preprocessing APIs do not operate on tensors and are not recommended for new code. Prefer loading data with either tf.keras.utils.text_dataset_from_directory or tf.keras.utils.image_dataset_from_directory, and then transforming the output tf.data.Dataset with preprocessing layers. These approaches will offer better performance and intergration with the broader Tensorflow ecosystem. For more information, see the tutorials for [loading text]( https://www.tensorflow.org/tutorials/load_data/text), [loading images]( https://www.tensorflow.org/tutorials/load_data/images), and [augmenting images]( https://www.tensorflow.org/tutorials/images/data_augmentation), as well as the [preprocessing layer guide]( https://www.tensorflow.org/guide/keras/preprocessing_layers). What I want to do in future I do not know how the numbers ‘1536’ in traits and ‘813’ in random selection in pictures come. Maybe it is related to model training or predicting. I should find out it some day and it can help me to know about ML. title: Current clustering method shows results unstably.[Wrong!]tags: - daily work - image clustering - kailancategories: - diariesdate: 05-24-2023What I have done today Installed R packages inborutils and devtools for downloading the large file of 2022SE from zenodo, but the file had been downloaded by zenodo_get and wget. Considering devtools is a useful package for R, it does not seem to waste time for me. Compared trees of outgroup with different parameters and all of them are different. The whole dataset is still downloading because the network connection of zenodo is not stably. Symmetric difference had been calculated by paup with command treedist and the normD (normalized different, maybe?) within inceptionResnetV2 model was 1, the largest number. It is not a acceptable result. The SSI calculation in supplemental file 3 needs a special input format of data contained node information of trees, which may be built by paup. Prepared for trip of Shennongjia in Hubei. What I need to do next I do not understand the meaning of ‘random consensus tree’ and ‘consensus tree’ in the article. title: Use whole dataset for testing image clusteringtags: - daily work - image clustering - kailancategories: - diariesdate: 05-28-2023What I have done today Downloaded the whole dataset contain 3792 pictures into $DIR/raw and disable its write permission.Renamed files to use ‘_‘ to replace the spaces in files’ name by rename \\_ *. Used a shell script to re-organize pictures by species: 12345678910111213141516171819#! /usr/bin/zshDIR=`pwd`# create species-based organized directory if it is not existif [ ! -e $DIR/organized ]then mkdir $DIR/organized cd $DIR/organized mkdir `ls $DIR/raw | awk -F _ &#x27;&#123;printf &quot;%s_%s\\n&quot;, $1,$2 &#125;&#x27; | uniq`ficd $DIR;for i in `ls $DIR/raw`do spname=`echo $i | awk -F _ &#x27;&#123;printf &quot;%s_%s&quot;, $1,$2&#125;&#x27;` cp $DIR/raw/$i $DIR/organized/$spnamedone; Used a shell script for checking if the copy result contain all pictures. 123456s=0; for i in `ls organized\\`; do ((s+=`ls organized/$i| wc -l`)); done;echo $s; Used the whole dataset to extract traits and build phylogeny tree with python script. The dataset contain some broken pictures and python cannot read, so use script from here to find them out and remove them. I found that the model tree is named &lt;model_name&gt;.nexus and the random tree is named random.nexus and I was wrong that I placed the model tree under the ../ directory, so I had always used the random trees for checking. I checked the model trees build by the whole dataset, results can be found below: Consensus tree of 100 trees built by inceptionResnetV2 model Consensus tree of 1011 trees built by inceptionResnetV2 model “At each internal node is an indication of how often the corresponding group (meaning all taxa descending from that internal node) was found in the set of all trees. (Numbers are percentages).” ref What I need to do next Build 10000 trees in one week and check it. title: Continuous traits phylogenetic analysis software installationtags: - daily work - continuous trait phylogenetic analysis - kailan - image clusteringcategories: - diariesdate: 05-30-2023What I have done today Continuous traits do not appear to be analysis by mrbayes software so I searched other methods and list below: Beast 2, which is re-write from Beast 1.x and contain more model than version 1. It also can be called in program R. see R package Phytools for data simulation and modeling. see RevBayes. see R package evorates. see (Model for traits evolutionary rates) So, the two useful software are Beast 2 and RevBayes. It is time to install bayesian analysis softwares and openmpi. Installed evorates by devtools. Installed openmpi, mrbayes by brew and complied revbayes in mpi version locally. Complied mpich locally but I do not know how to use it with mrbayes and revbayes. What I need to do next Approach Image-based data as continuous traits for phylogenetic analysis title: Bayesian anaylisis with continuous traits extracted from imagetags: - daily work - kailan - continuous trait phylogenetic analysiscategories: - diariesdate: 06-05-2023What I have done today Done with using continuous traits using the Revbayes script from here. Switched the .csv format file of traits into .nexus format for bayesian analysis with script of myself. Re-organize csv files and use script for directly switching and script for switching selected number pictures and script for calculate the mean traits. Bayesian phylogenetic tree had been built using average continuous traits of 26 species. Phylogenetic tree of 26 species using average traits of image 4. Re-trained inceptionResnetV2 model with 499x499 pix pictures instead of 299. What I need to do next Arithmetic average cannot represent the average level of image features. I should use another method. Evolution model of continuous trait can or cannot be change for better use of image? Are the traits extracted by models the ones I want? Should I train, even build, my own model? Or should I use model for trait extraction? The distance in the kailan-article, what exactly mean in physical or biological? 2023&#x2F;12&#x2F;12 (8.5h)Doing tasks finish phylobayes for gdl(0.5h) Done tasks P3 GTR for gdl (6.5h) download statistic_exercise (0.2h) install rust (0.1h) test rust (1.0h) config todo-daily for my blog (0.1h) config todo&#x2F;rust for my blog(0.1h) Todo tasks in this week (On Friday, next week scheduled tasks) install conda or mamba again find a rust-base blog engine understanding the statistic_exercise and done with it build a molecular data pippe line approach learning model adding and config in revbayes make a model list for revbayes P3 reading papers about 本质过程 Memo &amp; Comments 2023&#x2F;12&#x2F;14 (7.0h)Doing tasks test topology for gdl (2.0h) Done tasks finish phylobayes for gdl (1.0h) install conda or mamba again (0.5h) find a rust-base blog engine (0.5h) understanding the statistic_exercise and done with it (3.0h) Todo tasks in this week (On Friday, next week scheduled tasks) build a molecular data pippe line approach learning model adding and config in revbayes make a model list for revbayes P3 reading papers about 本质过程 write MH within gibbs script in R Memo &amp; Comments 2023&#x2F;12&#x2F;15 (7.0h)Doing tasks write MH within gibbs script in R (3.0h) Done tasks test topology for gdl (1.0h) install a theme for zola (1.0h) learn about MCMC and MH&#x2F;gibbs in R (2.0h) Todo tasks in this week (On Friday, next week scheduled tasks) build a molecular data pippe line approach learning model adding and config in revbayes make a model list for revbayes P3 reading papers about 本质过程 switch my blog to zola Memo &amp; Comments 2023&#x2F;12&#x2F;18 (12.5h)Doing tasks build a molecular data pippe line approach (1.0h) learning model adding and config in revbayes (1.0h) Done tasks write MH within gibbs script in R (3.0h) switch my blog to zola (0.5h) solve the problem with object function (2.0h) complete MH gibbs in R with GPT&#96;s help (5.0h) Todo tasks in this week (On Friday, next week scheduled tasks) make a model list for revbayes P3 reading papers about 本质过程 Memo &amp; Comments 2023&#x2F;12&#x2F;19 (8.0h)Doing tasks learning model adding and config in revbayes (1.0h) reading papers about 本质过程 (5.0h) Done tasks 本质过程思路整理 (2.0h) Todo tasks in this week (On Friday, next week scheduled tasks) build a molecular data pippe line approach make a model list for revbayes P3 输入文件格式，即数据来源 构建0演化，然后逐步添加变量因素 看一下共同个体&#x2F;共同物种概念方面的内容，和MSC是不是一样的 Memo &amp; Comments 2023&#x2F;12&#x2F;20 (0.0h)Doing tasksDone tasksTodo tasks in this week (On Friday, next week scheduled tasks) build a molecular data pippe line approach learning model adding and config in revbayes make a model list for revbayes P3 reading papers about 本质过程 输入文件格式，即数据来源 构建0演化，然后逐步添加变量因素 看一下共同个体&#x2F;共同物种概念方面的内容，和MSC是不是一样的 Memo &amp; Comments 2023&#x2F;12&#x2F;26 (5.0h)Doing tasksDone tasks leafhopper infer_base P3 (5.0h) Todo tasks in this week (On Friday, next week scheduled tasks) build a molecular data pippe line approach learning model adding and config in revbayes make a model list for revbayes P3 reading papers about 本质过程 输入文件格式，即数据来源 构建0演化，然后逐步添加变量因素 看一下共同个体&#x2F;共同物种概念方面的内容，和MSC是不是一样的 group will ppt model build and calculate Memo &amp; Comments 2023&#x2F;12&#x2F;27 (0.0h)Doing tasksDone tasks group will ppt Todo tasks in this week (On Friday, next week scheduled tasks) build a molecular data pippe line approach learning model adding and config in revbayes make a model list for revbayes P3 reading papers about 本质过程 输入文件格式，即数据来源 构建0演化，然后逐步添加变量因素 看一下共同个体&#x2F;共同物种概念方面的内容，和MSC是不是一样的 model build and calculate Memo &amp; Comments 2023&#x2F;12&#x2F;28 (0.0h)Doing tasksDone tasks 看一下共同个体&#x2F;共同物种概念方面的内容，和MSC是不是一样的 Todo tasks in this week (On Friday, next week scheduled tasks) build a molecular data pippe line approach learning model adding and config in revbayes make a model list for revbayes P3 reading papers about 本质过程 输入文件格式，即数据来源 构建0演化，然后逐步添加变量因素 model build and calculate Memo &amp; Comments 2023&#x2F;12&#x2F;29 (0.0h)Doing tasksDone tasksTodo tasks in this week (On Friday, next week scheduled tasks) build a molecular data pippe line approach learning model adding and config in revbayes make a model list for revbayes P3 reading papers about 本质过程 输入文件格式，即数据来源 构建0演化，然后逐步添加变量因素 model build and calculate 完整描述我的模型 描述计算方法 实践一下然后调整 Memo &amp; Comments 2024&#x2F;01&#x2F;11 (0.0h)Doing tasksDone tasksTodo tasks in this week (On Friday, next week scheduled tasks) build a molecular data pippe line approach learning model adding and config in revbayes make a model list for revbayes P3 reading papers about 本质过程 输入文件格式，即数据来源 构建0演化，然后逐步添加变量因素 model build and calculate 完整描述我的模型 描述计算方法 实践一下然后调整 learning mcmctree and beast for molecular dating principle build my model in notebook Memo &amp; Comments 2024&#x2F;01&#x2F;23 (0.0h)Doing tasksDone tasksTodo tasks in this week (On Friday, next week scheduled tasks) build a molecular data pippe line approach learning model adding and config in revbayes make a model list for revbayes P3 reading papers about 本质过程 输入文件格式，即数据来源 构建0演化，然后逐步添加变量因素 model build and calculate 完整描述我的模型 描述计算方法 实践一下然后调整 learning mcmctree and beast for molecular dating principle build my model in notebook ascp Memo &amp; Comments 2024&#x2F;01&#x2F;31 (0.0h)Doing tasksDone tasksTodo tasks in this week (On Friday, next week scheduled tasks) build a molecular data pippe line approach learning model adding and config in revbayes make a model list for revbayes P3 reading papers about 本质过程 输入文件格式，即数据来源 构建0演化，然后逐步添加变量因素 model build and calculate 完整描述我的模型 描述计算方法 实践一下然后调整 learning mcmctree and beast for molecular dating principle build my model in notebook ascp 批量下载基因 Memo &amp; Comments","categories":[{"name":"diaries","slug":"diaries","permalink":"http://zhangdeweb.site/categories/diaries/"}],"tags":[{"name":"daily","slug":"daily","permalink":"http://zhangdeweb.site/tags/daily/"},{"name":"work","slug":"work","permalink":"http://zhangdeweb.site/tags/work/"}]},{"title":"叶蝉采集经验总结及之所思","slug":"shennongjia2","date":"2023-06-15T16:00:00.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2023/06/16/shennongjia2/","link":"","permalink":"http://zhangdeweb.site/2023/06/16/shennongjia2/","excerpt":"","text":"在神农架的采集以叶蝉为重点类群。在连续数日的采集工作中，我发现若想提高采集效率，增加采集到的叶蝉种类和个体数量，需要一定的技巧。我将本次采集总结的经验、想法在此记述，并随时补充、更改，以提醒自己，方便后人。 思考 根据做叶蝉跳跃结构的师姐所述，以及前人文献记载，推断沫蝉跳跃能力强于叶蝉，而沫蝉若虫隐蔽生活，不具备跳跃能力。沫蝉多发现于草丛间，而叶蝉分布在草丛、灌木和树梢，飞虱多发现于树梢，因此考虑在头喙亚目中，跳跃能力或许与飞行能力互补，是两种不同的进化策略，对这两种策略的选择和演化或许是由食性驱动的。以上观察基于经验，没有数据和历史文献支撑，需要进一步查阅资料和事实证据。 同种叶蝉不同海拔貌似存在一点体型上的差异，不知道是偶然现象，还是观察偏差，还是误判，还是确有此事。需要记录好海拔信息，回去仔细测量。 工具传统吸虫管需要用到嘴巴，在采集中无法添加乙醇或乙酸乙酯杀死叶蝉，大体型叶蝉在管中跳跃会踢坏其他标本。本次采集我试用自己改造的吸虫管，利用洗耳球提供压力，利用单向气阀控制气体流动方向，形成负压，提供吸力。 经过几天的使用，总体而言，吸力充足，效率与传统吸虫管相当，且可以随时用酒精杀死管中叶蝉。但仍然存在一些可以改进的地方： 在吸气口固定棉花吸收多余酒精，可能导致酒精被吸入洗耳球，造成酒精少量浪费和吸力短暂下降使昆虫逃逸，考虑在其他位置固定棉花，如进气口。 使用100ml离心管时直径略粗，长时间操作时会略吃力，50ml离心管相比之下使用起来更轻松，但在虫量很大时有可能堵住吸虫管，考虑每天准备两个50ml管，或进一步改造改造管体使吸虫工具和存储工具分离（吸虫管中不能有积液，采集过程中叶蝉会腐烂或干燥导致变脆易损，采集、存储相分离可以避免该问题）。 吸管直径略细，部分叶蝉头宽大于吸管内径，无法被吸入，如果增大吸管内径，可能导致吸力下降。而在实际使用中，吸虫管的吸力并不是采集叶蝉的主要动力，而是提供一个气流让叶蝉跳跃，叶蝉跳跃时的动力已经足够使叶蝉进入吸虫管，具体数值待测。 金属吸管不透明，如果有虫卡在管中不及时发现，会造成堵塞或昆虫逃逸，而玻璃吸管易碎，应考虑用强度高的玻璃或透明的复合材质。 导气管应考虑使用更硬的材质及弯头导管，否则气道会因形变堵塞。 软胶塞应考虑外包围固定或直筒型，倒台型软塞会夹住虫体导致标本破损。 如果有条件应考虑固定吸虫管与洗耳球相对位置，或加入人体工程学因素，使其更容易抓握和用力。 捕虫网需短小结实最长两米，网圈直径不超过40，直径太大伸不进树枝里。 扫网扫网是叶蝉采集的最关键步骤，其中路线选择和时间分配是影响采集效率的最主要因素。 叶蝉聚集分布于寄主上，受惊会跳跃到其他位置。根据这一习性和日前采集经验可知，采集路线应选为人迹稀少的林下小道，车辆较少且两侧有丰富植被的道路，干涸的河道，以及田间小路等。 在行进路程中，应采用广泛观察，重点收集的策略。依据不同的环境，整个行进路线会被分为不同的区块。对每个区块内的不同寄主进行简单快速的扫网，观察网内情况，估计附近叶蝉分布。根据个体数量、种类情况判断是否在该区域进行详细、大范围的采集。 每次仔细扫网、收网约需要耗时15分钟，每次简单扫网、估计情况约耗时2分钟。根据日前统计，根据采集情况不同，我每天可以约详细扫网25-35次，总扫网次数在45-50次，保证扫网次数可以控制好扫网和赶路二者的时间分配，避免出现采集只赶路不采虫的情况发生。 若昆虫种类丰富，数量繁多，则扫网次数多，赶路时间少，行进路程短，对下一区域期望高，因此在估计情况时应更倾向于放弃扫网，增加行进路程；若昆虫种类数量少，对下一区域昆虫分布期望低，则应在一段时间的赶路后，在某一相对丰富区域持续扫网。 总体而言，草丛中物种和个体数量均大于灌木和树梢，但草丛中种类与灌木、树梢中不同，因此对草丛、灌木、树梢均要观察采集，以进一步估计情况。 在蒿草丛，若沫蝉丰富度极高，则叶蝉种类较少，应酌情放弃该区域的采集。 叶蝉跳跃前需要有准备动作，其白天多于叶背休息，因此受到惊吓时倾向于向下掉落。根据这一特点，扫网时应从下向上，左右交替挥网。 灯诱 叶蝉远距离飞行能力弱，在灯诱时主要吸引周边近距离植被上的叶蝉。同时，叶蝉是相对弱小的类群，当灯诱布上昆虫密集时，叶蝉会受惊逃跑。因此，应在天黑前半小时架灯，此时灯光吸引能力较弱，飞蛾、金龟等大体积昆虫不会上灯，叶蝉会集中上灯而不会被惊扰。天黑后半小时，其他昆虫上灯，叶蝉陆续逃逸到附近草丛，采集完布上叶蝉可在周边扫网，到十二点左右布上昆虫不再活跃，叶蝉会再次上灯，可以再次在布上采集。 气温15度以上时，叶蝉才具备上灯的活动能力，若低于15度应以扫网为主。","categories":[{"name":"感想","slug":"感想","permalink":"http://zhangdeweb.site/categories/%E6%84%9F%E6%83%B3/"}],"tags":[{"name":"感想","slug":"感想","permalink":"http://zhangdeweb.site/tags/%E6%84%9F%E6%83%B3/"}]},{"title":"记神农架联合采集之有感","slug":"shennongjia1","date":"2023-06-06T16:00:00.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2023/06/07/shennongjia1/","link":"","permalink":"http://zhangdeweb.site/2023/06/07/shennongjia1/","excerpt":"","text":"今天是正式采集的第二天，不幸遇到连绵阴雨。雨水打湿植被，叶蝉贴在植物表面，很难通过扫网的方式采到。从早上到下午三点，也只有寥寥半瓶子的“战果”，而衣服鞋子却早已浸湿。奈何只得回到住所整理标本和器具，做些清闲事。本次神农架之行，是我近一年以来第一次外出采集，这对我而言有一些特殊的意义，借此闲暇，回顾过往，抒怀一二。 我是非常喜欢外出的。在本科时候，一遇节假日便会组织三五好友，带上捕虫网、望远镜，到秦岭里采集昆虫，观察鸟类。我们几个自然爱好者，各自有感兴趣的类群，在一起交流自然趣闻，不亦乐乎。从读上研究生开始，采集更多的变成了一种工作，对昆虫的收集从兴趣使然变成了责任和压力在驱动。曾经，我采集昆虫或是因为好看，或是因为稀有，或是可以饲养，是纯粹而简单的爱好。我把它们做成标本，摆在玻璃盒子里，仿佛那些都是我的勋章。读研之后，我不得不去更深入地思考这些神奇物种身上蕴藏的科学问题。这让我感到茫然，虽然我仍然喜爱外出，积极的采集昆虫，但我越是仔细的观察它们，越是感觉自己仿佛在注视深渊。我感到恐惧和虚无——我曾以为我已经足够了解它们，但实际上我一无所知。在外采集的每一天都令我不安，令我紧张，我不能再盲目的抓虫子。 之后，我开始逐渐把我的精力放在读书上。我看了许多文献，除了领域内的，还有很多相关或间接相关领域的文献。读书的过程和采集一样，需要耐得住寂寞，然后就会发现各种各样的惊喜，让人目不暇接。通过和导师、朋友们不断的交流，我逐渐理清了一些思路，模模糊糊地找到一个方向，而到这个时候，时间已经过去了整整一年。莫名的紧迫感几乎快要淹没我，我需要工作，我需要忙碌起来，似乎只有忙碌起来，我的时间才具有价值，我才真正的存在于世界上。恰好得知有机会来到神农架采集，我便连忙报名准备，得益于师姐和老师们的支持，有幸加入此行。 我给自己定过一个小目标，未来四年，我希望可以采集到10000头类群内的标本。定这个目标的初衷并不是盲目的追求数量，而是希望自己多采集，多出野外，不要懈怠，不要蜷缩在一个屋子里自说自话。因此这次采集我做了十足的准备，我相信压力之下必有成果。而非常非常幸运的是，我所在的采集小组其他成员也都非常热爱自然，与他们同行交流，我仿佛回到了本科阶段那个纯粹而快乐的野采集。大自然重新在我的眼里活泼了起来，一切是那么的有趣，它不陌生，不可怕，大自然是我的导师，是我的挚友，它在引导我探求更深层次的科学问题。我感受到前所未有的愉快，采集再次变成了令我快乐的事。采集途中虽有一些挫折，会令我有些许沮丧，但是想到还有很多事情可以做，有很多事情需要我去做，我就又有了力量继续工作。 这次采集让我再次拥抱大自然，对我来说是一个新的开始。我认为我找到了压力和兴趣爱好之间巧妙的平衡，我不会沉溺在迷茫或者焦虑里不能自拔。以后，我会遇到新的人，有新的故事，看新的风景，抓新的虫子，发现新的问题。我一如既往地热爱大自然，但我希望以后我可以作为一名学者，而不仅仅是爱好者去接触它。希望自己继续努力！ 张钧儒2023年6月7日 记于神农架小龙潭金丝猴姊妹峰繁育基地","categories":[{"name":"感想","slug":"感想","permalink":"http://zhangdeweb.site/categories/%E6%84%9F%E6%83%B3/"}],"tags":[{"name":"感想","slug":"感想","permalink":"http://zhangdeweb.site/tags/%E6%84%9F%E6%83%B3/"}]},{"title":"ZMK keyboard firmware usage","slug":"ZMK","date":"2023-05-08T16:00:00.000Z","updated":"2025-01-01T16:12:13.096Z","comments":true,"path":"2023/05/09/ZMK/","link":"","permalink":"http://zhangdeweb.site/2023/05/09/ZMK/","excerpt":"","text":"I have purchased an open source split keyboard with zmk firmware. Unlike those using qmk, the keymaps of this one cannot be changed simply and directly using via&#x2F;vial software, it have to build firmware myself. I do not have t experience with this and after my first successful attempt, I decided to document it here. Normal usage This keyboard can be connected to PC in two ways. USB connection or Blue-tooth connection. If you want to connect to PC with cables, just connect the left hand keyboard to PC with USB C cable. If you want to connect device with Blue-tooth, press layer 2 button (which is on the bottom of right hand) and click number 1 to 5 (which means you can connect 5 different device with blue tooth), and then your device can found the keyboard by blue tooth name ‘Sofle’. If the blue tooth connect failed, try clearing the blue tooth message in both keyboard and your PC. Delete the ‘Sofle’ device in PC blue tooth setting and press layer 2 + ESC on the keyboard. The location of ESC in layer 2 is BT_CLR button. The location of ‘TAB’ in layer 3 is the power button, which means you can turn on&#x2F;off with pressing layer 1 + layer 2 + tab at the same time. Check and set the keymap in Github Fork this repo (https://github.com/curiosusJR/YoungMan_DIY_SOFLE_CFG) to your own github. Open https://nickcoutsos.github.io/keymap-editor/ with web-browser. Select the source in Github and login your account. Select the repo you just forked only, not all the repo you have, for the keymap-editor. It will be shown my keymap setting in the website. Change the setting as you will. Select the layer first, then select the key you want to change. Click the &amp;kp can change the group of keys. Different groups shows different keys. The normal keyboard press group is &amp;kp. You can select any key you want at any location. The knob can be click or rotation. The rotation function setting is on the bottom of keyboards. Tips: The layer 0 is the default and the main layer you will use. Use &amp;mo for changing layer during pressing the button, use &amp;lt for short tapping in normal type and pressing in layer changing type or use &amp;to for turning into new layer when tap the button. After finishing the setting, click Save changes button on the top of website and commit your changing to your repo. Check the commit exists. Build the firmware file in Github Click Actions on your own Github repo. Enable the actions and run the ‘Build’ workflow.Wait for a second till it’s done. When it shows a green check mark, click the ‘Build’ and download the firmware file at the bottom. Unzip it and delete the setting_reset-nrfmicro_13-zmk.uf2 file. Keep the right hand and the left hand firmware file for use. Reset the firmware in the keyboard First, connect keyboard to PC with USB C cable. The right hand keyboard and the left one should be reset separately, so connect the right one first. Press the reset button (the small white one under the screen) twice quickly and the computer will recognize a flash drive which is the keyboard itself. There should be three files in the flash drive just detected. Copy this file into the root directory of that flash drive. Now there are four files in it. Eject that flash drive. Unplug the USB cable and reconnect it. Repeat step 2 and check the drive. There should be still three files in it. Copy the right hand .uf2 file into the right hand keyboard and eject the drive. Up to now, the firmware of right hand has been update completed. Do it same to the left one. Just remember that copy this file, not the file downloaded from github, to both left and right hand keyboard before copying the solfe_*.uf2 file. After updating the firmware of both left and right hands, press BT_CLR button you set to clear the blue-tooth message. Press the reset button of both right and left hand keyboard in the same time. Check the connection between two hands and connection to PC. Enjoy your new keymaps! 😄","categories":[{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"}],"tags":[{"name":"zmk","slug":"zmk","permalink":"http://zhangdeweb.site/tags/zmk/"},{"name":"keyboard","slug":"keyboard","permalink":"http://zhangdeweb.site/tags/keyboard/"}]},{"title":"R语言环境变量","slug":"Renv","date":"2023-04-07T12:47:49.000Z","updated":"2025-01-01T16:12:13.096Z","comments":true,"path":"2023/04/07/Renv/","link":"","permalink":"http://zhangdeweb.site/2023/04/07/Renv/","excerpt":"","text":"之前也遇到过R语言环境变量配置方面的问题，最近又遇到，每次都要花一些时间搜索，这种重复工作很令人沮丧，在此简单记录： 用R markdown转bemaer tex的时候，我希望能对tex文件直接编辑，使其更灵活、更符合我的需求，而转换中用的knit会自动删除tex文件，此处需要在头部的yaml中声明。这里Rstudio会自动翻译yaml头部声明，提供可视化的option选项，在knit旁边的设置（齿轮图标）中，然而，只有当yaml格式符合标准时，option选项才会出现，如果没有option选项，请重新生成默认的yaml头部声明，此时设置菜单中会出现“output options”，其中在“Advanced”选项卡中勾选“Keep tex source file”选项。该操作等价于头部声明： 123output: beamer_presentation: keep_tex: yes","categories":[{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"}],"tags":[{"name":"config","slug":"config","permalink":"http://zhangdeweb.site/tags/config/"},{"name":"Rstudio","slug":"Rstudio","permalink":"http://zhangdeweb.site/tags/Rstudio/"},{"name":"Renv","slug":"Renv","permalink":"http://zhangdeweb.site/tags/Renv/"},{"name":"Rmarkdown","slug":"Rmarkdown","permalink":"http://zhangdeweb.site/tags/Rmarkdown/"},{"name":"Beamer","slug":"Beamer","permalink":"http://zhangdeweb.site/tags/Beamer/"}]},{"title":"Archlinux installation and personal configuration","slug":"archlinux","date":"2023-04-07T12:47:49.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2023/04/07/archlinux/","link":"","permalink":"http://zhangdeweb.site/2023/04/07/archlinux/","excerpt":"","text":"Archlinux, just like their official website said, is “a lightweight and flexible Linux® distribution that tries to Keep It Simple”. It is better for personal use than Ubuntu which is running on my tower server. I have tried Windows 10&#x2F;11 and MacOS on my personal laptop, but none of them can make me satisfy. After trying the tilling windows manager such as i3wm on Ubuntu, I don’t want to back to other desktop environment. Heard that arch has an excellent package manager, so I decide to have a try. In view of the complexity of the installation process, it is hereby documented as follows. 2023.4.7 to be continue","categories":[{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"}],"tags":[{"name":"config","slug":"config","permalink":"http://zhangdeweb.site/tags/config/"},{"name":"linux","slug":"linux","permalink":"http://zhangdeweb.site/tags/linux/"},{"name":"archlinux","slug":"archlinux","permalink":"http://zhangdeweb.site/tags/archlinux/"}]},{"title":"系统发育中快速/缓慢收敛的mcmc数据是什么样的","slug":"convergence","date":"2023-04-07T12:47:49.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2023/04/07/convergence/","link":"","permalink":"http://zhangdeweb.site/2023/04/07/convergence/","excerpt":"","text":"快速收敛的数据满足信号一致： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 49 50T_01 ACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_02 AACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_03 AAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_04 AAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_05 AAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_06 AAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_07 AAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_08 AAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_09 AAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_10 AAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_11 AAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_12 AAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_13 AAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_14 AAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_15 AAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_16 AAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_17 AAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_18 AAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_19 AAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_20 AAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_21 AAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCCT_22 AAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCCT_23 AAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCCT_24 AAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCCT_25 AAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCCT_26 AAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCCT_27 AAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCCT_28 AAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCCT_29 AAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCCT_30 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCCT_31 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCCT_32 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCCT_33 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCCT_34 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCT_35 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCCT_36 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCCT_37 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCCT_38 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCCT_39 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCCT_40 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCCT_41 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCCT_42 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCCT_43 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCCT_44 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCCT_45 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCCT_46 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCCT_47 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCT_48 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCT_49 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC 该数据文件可以在Mrbayes中快速收敛： 12345678910111213141516171819202122232425262728Chain results (1000000 generations requested): 0 -- [-348856.375] [...7 remote chains...] 1000 -- [-74447.065] [...7 remote chains...] -- 0:00:00 2000 -- [-45964.399] [...7 remote chains...] -- 0:08:19 3000 -- (-42577.505) [...7 remote chains...] -- 0:11:04 4000 -- (-41096.351) [...7 remote chains...] -- 0:08:18 5000 -- [-40706.751] [...7 remote chains...] -- 0:09:57 Average standard deviation of split frequencies: 0.113752 6000 -- (-40614.814) [...7 remote chains...] -- 0:11:02 7000 -- (-40580.207) [...7 remote chains...] -- 0:09:27 8000 -- (-40543.464) [...7 remote chains...] -- 0:10:20 9000 -- (-40526.129) [...7 remote chains...] -- 0:11:00 10000 -- (-40498.073) [...7 remote chains...] -- 0:09:54 Average standard deviation of split frequencies: 0.003343 11000 -- (-40515.419) [...7 remote chains...] -- 0:10:29 12000 -- (-40512.721) [...7 remote chains...] -- 0:10:58 13000 -- (-40518.564) [...7 remote chains...] -- 0:11:23 14000 -- (-40517.432) [...7 remote chains...] -- 0:10:33 15000 -- (-40498.062) [...7 remote chains...] -- 0:10:56 Average standard deviation of split frequencies: 0.000000 16000 -- (-40503.398) [...7 remote chains...] -- 0:11:16 如果此时，将该文件中otu的序号打乱，生成新文件，例如简单交换前25行和后25行序号位置，此时新文件支持和原来截然不同的系统发育树（T1和T49从需要突变50位点变成需要突变1位点），如果将二者合并成新文件再随机选择其中50列，即可得到缓慢收敛的数据： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 49 50T_01 AAAACCAACCCCCACCCCCCCCCCCACCCACCCCCCCCCCCCACCCCCCCT_02 AAAACCAACCCCCACCCCCCCCCCCACCCACCCCCCCCCCCCACCCCCCCT_03 AAAACCAACCCCCACCCCACCCCCCACCCACCCCCCCCCCCCACCCCCCCT_04 AAAACCAACCCCCACCCCACCCCCCACCCACCCCCCCCCCCCACCCCCCCT_05 AAAACCAACCCCCACCCCACCCCCCACCCACCCCCCCCCCCCACCCCCCCT_06 AAAACCAACCCCCACCCCACCCCCCCCCCACCCCCACCCCCCACCCCCCCT_07 AAAACCAACCCCCACCCCAACCCCCCCCCACCCACACCCCCCACCCCCCCT_08 AAAACCAACCCCCACCCCAACCCCCCCCCACCCACACCCCCCACCCCCCCT_09 CAAACCAACCCACACCCCAACCCCCCCCCACCCACACCCCCCACCCCCCCT_10 CAAACCAACCCACACCCCAACCCCCCCCCACCCACACCCCCCACCCCCCCT_11 CAAACCAACCCACACCCCAACCCCCCCCCACCCACACCCACCACCCACCCT_12 CAAACCAACCCACACCCCAACCCCCCCCCACCCACACCCACCACCCACCCT_13 CAAACCAACCCACACCCCAACCCCCCCCCACCCACACCCACCACCCACCCT_14 CAAACCAACCCACACCACAACCCCCCCCCACCCACACCCACCACCCACCCT_15 CACACCAACCCACACCACAACCCCCCCCCACCCACACCCACCACCCACCCT_16 CACACCAACCCACACCACAACCCCCCCCCACCCACACCCACCACCCACCCT_17 CACACCAACCCACACCACAACCCCCCCCCACCCACACCCACCACCCACCCT_18 CACACCAACCCACCCCACAACCCCCCCCCACCCACACCCACCACCCACCCT_19 CACACCAACCCACCCCACAACCCCCCCCCACCCACACCCACCACCCACCCT_20 CACACCAACCCACCCCACAACCCACCCCCACCAACACCCACCCCCCACCCT_21 CACACCAACCCACCCCACAACCCACCCCCACCAACACCCACCCCCCACCCT_22 CACACCAACCCACCCCACAACCCACCCCCACCAACACCCACCCCCCACCCT_23 CACACCACCCCACCCCACAACCCACCCCCACCAACACCCACCCCCCACCCT_24 CACCCCCCCCCACCCCACAACCCACCCCCACCAACACCCACCCCCCACCCT_25 CCCCCCCCCCCACCCCACAACCCACCCCCCCCAACACCAACCCCCCACCCT_26 AAAACAAACCCAAACCAAAAACAAAACCCACCAAAACCAACCACAAAAAAT_27 AAAACAAACCCAAACCAAAAACAAAACCCACCAAAACCAACCACAAAAAAT_28 AAAACAAACCAAAACCAAAAACAAAACCCACCAACACCAACCACAAAAAAT_29 AAAAAAAACCAAAACCACAAACAAAACCCAACAACACCAACCACAAAAAAT_30 AAAAAAAACCAAAACCACAAACAAAACCCAACAACACCAACCACAAAAAAT_31 AAAAAAAACCAAAACCACAAACCAAACCCAACAACACCAACCACAAAAAAT_32 AAAAAAAACCAAAACCACAAACCAAACACAACAACACCAACCACCAAAAAT_33 AAAAACAACCAAAACCACAAACCACACACAACAACACCAACCACCAAAACT_34 AAAAACAACCAAAACCACAAACCACAAACAACAACACCAACCACCAAAACT_35 AAAAACAAACAAAACCACAAACCACAAACAACAACACCAACCACCAAAACT_36 AAAAACAAACAAAACCACAAACCACAAACAACAACACCAACAACCAAAACT_37 AAAAACAAACAAAACCACAAAACACAAACAACAACACAAACAACCAAAACT_38 AAAAACAAACAAAACCACAAAACACAAACAACAACACAAACAACCAAAACT_39 AAAAACAAACAAAACCACAAAACACAAACAACAACACAAACAACCAAAACT_40 AAAAACAAACAAAACAACAAAACACAAACAACAACACAAACAACCAAAACT_41 AAAAACAAACAACACAACAAAACACAAACAACAACACAAACAAACAAACCT_42 AAAAACAAACAACACAACAAAACACAAACAACAACACAAACAAACAAACCT_43 AAAAACAAACAACACAACAAAACACAAACAACAACACAAACAAACAAACCT_44 AAAAACAAACAACACAACAAAACACAAACAACAACACAAACAAACAAACCT_45 AAAAACAAACAACAAAACAAAACACAAACAACAACACAAACAAACAAACCT_46 AAAAACAAACAACAAAACAACACACAAACAACAACAAAAACAAACAAACCT_47 AAAAACAAACAACAAAACAACACACAAACAACAACAAAAACAAACAAACCT_48 AAAAACAAACAACAAAACAACACACAAACAACAACAAAAAAAAACCAACCT_49 AAAAACAAACAACAAAACAACACACAAACAACAACAAAAAAAAACCACCC 这种具有强烈互斥的系统发育信号的数据集在多次迭代后仍然难以收敛： 12345678910111213141516171819Average standard deviation of split frequencies: 0.24221046000 -- (-9100.801) [...7 remote chains...] -- 0:12:2647000 -- (-9099.480) [...7 remote chains...] -- 0:12:3048000 -- (-9118.462) [...7 remote chains...] -- 0:12:3349000 -- (-9090.241) [...7 remote chains...] -- 0:12:3650000 -- (-9101.250) [...7 remote chains...] -- 0:12:40Average standard deviation of split frequencies: 0.24765251000 -- (-9105.160) [...7 remote chains...] -- 0:12:2452000 -- (-9101.906) [...7 remote chains...] -- 0:12:2753000 -- (-9106.679) [...7 remote chains...] -- 0:12:3054000 -- (-9105.292) [...7 remote chains...] -- 0:12:3355000 -- (-9104.724) [...7 remote chains...] -- 0:12:36Average standard deviation of split frequencies: 0.23650456000 -- (-9104.708) [...7 remote chains...] -- 0:12:21 造成难收敛的因素有很多，收敛性的评价也很困难。我的问额是，收敛性好的数据集是系统发育信号更好的数据集吗？不，是统计上一致的数据集，并不一定更好。同时该参数也不能完全评判收敛性，还应进一步考虑。","categories":[{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"}],"tags":[{"name":"convergence","slug":"convergence","permalink":"http://zhangdeweb.site/tags/convergence/"},{"name":"mcmc","slug":"mcmc","permalink":"http://zhangdeweb.site/tags/mcmc/"},{"name":"heter_test","slug":"heter-test","permalink":"http://zhangdeweb.site/tags/heter-test/"},{"name":"pps","slug":"pps","permalink":"http://zhangdeweb.site/tags/pps/"}]},{"title":"Measure insect male external genitalia by skelevision protocol","slug":"skelevision","date":"2023-04-03T12:47:49.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2023/04/03/skelevision/","link":"","permalink":"http://zhangdeweb.site/2023/04/03/skelevision/","excerpt":"","text":"Skelevision is a wonderful work which is a automatically protocol to measure birds bones using man-made labels. When I see those skeletal specimens, I think they are kind of similar to insect male external genitalia. I don’t have any experience with machine learning but thanks to the detailed README file of Skelevision, even it starts from installing conda, I can reproduce the whole protocol to my computer and learn it. Pictures PreparationI was fortunate to have the pictures from a leafhopper subfamily taken by Dr. Liang Zonglei. These *.tiff pictures’s name are unformatted so let’s begin with pics format converting and name formatting. Format convertingUsing ffmpeg to convert the pics format. Tiff files are scattered accross various levels of folders so we need a little script to handle this. Tips: If there are spaces in file name, use rename \\ _ **/* to convert spaces to ‘_‘. 12345678k=0;for i in `ls **/*.tif`; do ffmpeg -i $i $i.png; rm $i;-mv $i.png ../format_convert/$k.png((k=$k+1));done; Databasinghold on AnnotateJust like authors did in their research , I annotate pictures using VGG. I tried to annotate 6 pictures and estimate the time I should use to train. Add selected pictures to VGG and draw 5 different regions each picture. Press [Enter] to finish drawing the region or press [Esc] to cancel. Define 5 attributes.Select type as dropdown from the list of available options and define option values as shown in screenshot below. Click the regions and define annotations. In the end, export annotations in coco format. Use skelevision scripts shown in github to train pictures. cpu runs in 2 weeks. DATE: 2023.4.4 TO BE CONTINUE","categories":[{"name":"paper","slug":"paper","permalink":"http://zhangdeweb.site/categories/paper/"}],"tags":[{"name":"reproducification","slug":"reproducification","permalink":"http://zhangdeweb.site/tags/reproducification/"},{"name":"idea","slug":"idea","permalink":"http://zhangdeweb.site/tags/idea/"},{"name":"skelevision","slug":"skelevision","permalink":"http://zhangdeweb.site/tags/skelevision/"}]},{"title":"How to sync blogs between different devices","slug":"sync","date":"2023-04-01T09:07:49.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2023/04/01/sync/","link":"","permalink":"http://zhangdeweb.site/2023/04/01/sync/","excerpt":"","text":"build enviroment install node, git and hexo; git clone -b hexo https://github.com/curiosusJR/curiosusJR.github.io.git curiosus cd cuirosus &amp;&amp; sudo npm install # init config and packages github authentication configure github global setting with command git config --global user.name &quot;&quot; and git config --global user.email &quot;&quot; “hexo d” and “git push” commands need to authenticate github by token; only repo and workflow is necessary. NOTE: token should not be upload to remote repository, means .gitignore should include files below token for hexo: vim .&#x2F;_config.yml 123456deploy: type: git repository: https://&lt;token-for-hexo&gt;@github.com/&lt;user.name&gt;/&lt;repo.name&gt;.git branch: master name: &lt;user.name&gt; email: &lt;email-address&gt; token for git: vim .&#x2F;.git&#x2F;config 12[remote &quot;origin&quot;] url = https://&lt;token-for-hexo&gt;@github.com/&lt;user.name&gt;/&lt;repo.name&gt;.git valid .gitignore after changing it by delet local cache: 123git rm -r --cached .git add .git commit -m &#x27;update .gitignore&#x27; daily use git pull # for latest data git branch -vv # check branch matchup between local and remote hexo n “&lt;title&gt;“ # hexo g; hexo d; et.alNOTE: “hexo d” update the master branch contain website in github git add . &amp;&amp; git commit -m ““ &amp;&amp; git push # remember use command “git push” without “origin hexo” only when “git branch -vv” results a correct matchup.NOTE: git push up date hexo branch contain source data reference https://www.jianshu.com/p/85f455afcfcf https://www.jianshu.com/p/6ecb3adfefbd","categories":[{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://zhangdeweb.site/tags/blog/"},{"name":"sync","slug":"sync","permalink":"http://zhangdeweb.site/tags/sync/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-03-01T09:07:49.000Z","updated":"2025-01-01T16:12:13.100Z","comments":true,"path":"2023/03/01/hello-world/","link":"","permalink":"http://zhangdeweb.site/2023/03/01/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://zhangdeweb.site/tags/blog/"},{"name":"sync","slug":"sync","permalink":"http://zhangdeweb.site/tags/sync/"}]}],"categories":[{"name":"diaries","slug":"diaries","permalink":"http://zhangdeweb.site/categories/diaries/"},{"name":"TODO","slug":"diaries/TODO","permalink":"http://zhangdeweb.site/categories/diaries/TODO/"},{"name":"blogs","slug":"blogs","permalink":"http://zhangdeweb.site/categories/blogs/"},{"name":"TODO","slug":"TODO","permalink":"http://zhangdeweb.site/categories/TODO/"},{"name":"感想","slug":"感想","permalink":"http://zhangdeweb.site/categories/%E6%84%9F%E6%83%B3/"},{"name":"paper","slug":"paper","permalink":"http://zhangdeweb.site/categories/paper/"}],"tags":[{"name":"coffee","slug":"coffee","permalink":"http://zhangdeweb.site/tags/coffee/"},{"name":"taste_note","slug":"taste-note","permalink":"http://zhangdeweb.site/tags/taste-note/"},{"name":"daily","slug":"daily","permalink":"http://zhangdeweb.site/tags/daily/"},{"name":"work","slug":"work","permalink":"http://zhangdeweb.site/tags/work/"},{"name":"blog","slug":"blog","permalink":"http://zhangdeweb.site/tags/blog/"},{"name":"Wish-list","slug":"Wish-list","permalink":"http://zhangdeweb.site/tags/Wish-list/"},{"name":"TODO","slug":"TODO","permalink":"http://zhangdeweb.site/tags/TODO/"},{"name":"感想","slug":"感想","permalink":"http://zhangdeweb.site/tags/%E6%84%9F%E6%83%B3/"},{"name":"zmk","slug":"zmk","permalink":"http://zhangdeweb.site/tags/zmk/"},{"name":"keyboard","slug":"keyboard","permalink":"http://zhangdeweb.site/tags/keyboard/"},{"name":"config","slug":"config","permalink":"http://zhangdeweb.site/tags/config/"},{"name":"Rstudio","slug":"Rstudio","permalink":"http://zhangdeweb.site/tags/Rstudio/"},{"name":"Renv","slug":"Renv","permalink":"http://zhangdeweb.site/tags/Renv/"},{"name":"Rmarkdown","slug":"Rmarkdown","permalink":"http://zhangdeweb.site/tags/Rmarkdown/"},{"name":"Beamer","slug":"Beamer","permalink":"http://zhangdeweb.site/tags/Beamer/"},{"name":"linux","slug":"linux","permalink":"http://zhangdeweb.site/tags/linux/"},{"name":"archlinux","slug":"archlinux","permalink":"http://zhangdeweb.site/tags/archlinux/"},{"name":"convergence","slug":"convergence","permalink":"http://zhangdeweb.site/tags/convergence/"},{"name":"mcmc","slug":"mcmc","permalink":"http://zhangdeweb.site/tags/mcmc/"},{"name":"heter_test","slug":"heter-test","permalink":"http://zhangdeweb.site/tags/heter-test/"},{"name":"pps","slug":"pps","permalink":"http://zhangdeweb.site/tags/pps/"},{"name":"reproducification","slug":"reproducification","permalink":"http://zhangdeweb.site/tags/reproducification/"},{"name":"idea","slug":"idea","permalink":"http://zhangdeweb.site/tags/idea/"},{"name":"skelevision","slug":"skelevision","permalink":"http://zhangdeweb.site/tags/skelevision/"},{"name":"sync","slug":"sync","permalink":"http://zhangdeweb.site/tags/sync/"}]}